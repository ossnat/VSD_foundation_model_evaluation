{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMFu37hR0o6QlwA68mUFFU+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3a011b9f96a3460ba3cf3f734f88d04f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_24205e095ea14292b60a8775688fff9a",
              "IPY_MODEL_a36562b7bb084d608b1822d213ccbf75",
              "IPY_MODEL_105dfd7b796b4a4d8628f2549ce33126"
            ],
            "layout": "IPY_MODEL_4f9843756bbb47e1b1d93e5d607957ae"
          }
        },
        "24205e095ea14292b60a8775688fff9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d17e4b9871c14533928beda978182e8b",
            "placeholder": "​",
            "style": "IPY_MODEL_51ec61dd784b4cdca680b4fbf98292ff",
            "value": "config.json: "
          }
        },
        "a36562b7bb084d608b1822d213ccbf75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e56fddfb202b4ca2bb553f3cf59b25de",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2b4cbdc1ee0f4bf09a644d215284900b",
            "value": 1
          }
        },
        "105dfd7b796b4a4d8628f2549ce33126": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4db14d6c8dc0410ebcc99755616e860d",
            "placeholder": "​",
            "style": "IPY_MODEL_edc6ae75d94a455991a6596f8f1cb0cc",
            "value": " 4.19k/? [00:00&lt;00:00, 461kB/s]"
          }
        },
        "4f9843756bbb47e1b1d93e5d607957ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d17e4b9871c14533928beda978182e8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51ec61dd784b4cdca680b4fbf98292ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e56fddfb202b4ca2bb553f3cf59b25de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "2b4cbdc1ee0f4bf09a644d215284900b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4db14d6c8dc0410ebcc99755616e860d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edc6ae75d94a455991a6596f8f1cb0cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d6b7e842f0d440eb80dab462f6f46f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_758ca77b22cc4486bada6eb124173ab2",
              "IPY_MODEL_f3b3c39c5c924ecb8ccd5f2e9dcdf48b",
              "IPY_MODEL_85a1640a9fe44e89b6110a3f03b30736"
            ],
            "layout": "IPY_MODEL_cc4310128a584317a40b7d35be83e042"
          }
        },
        "758ca77b22cc4486bada6eb124173ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c0a1ed525194d2cab575e6889a44139",
            "placeholder": "​",
            "style": "IPY_MODEL_9168cfd6408243949fdcd40e21aeae7d",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "f3b3c39c5c924ecb8ccd5f2e9dcdf48b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_676a748a2e0443239cf3e4b336e04823",
            "max": 605247071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ab380126e4649458d32018c2483b118",
            "value": 605247071
          }
        },
        "85a1640a9fe44e89b6110a3f03b30736": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ea0d56aa7154e989d8bc6547f9711df",
            "placeholder": "​",
            "style": "IPY_MODEL_7749e3701a6544b095d6662c45350bec",
            "value": " 605M/605M [00:02&lt;00:00, 465MB/s]"
          }
        },
        "cc4310128a584317a40b7d35be83e042": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c0a1ed525194d2cab575e6889a44139": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9168cfd6408243949fdcd40e21aeae7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "676a748a2e0443239cf3e4b336e04823": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ab380126e4649458d32018c2483b118": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7ea0d56aa7154e989d8bc6547f9711df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7749e3701a6544b095d6662c45350bec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ef19ceb9f494d67bc27a63db634ef82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c6f18fed2c69415daf8f8afbdf6cfa53",
              "IPY_MODEL_7ba9714025134ccd94a61fce4b0efbc0",
              "IPY_MODEL_66aa98e9d960434396eb196db47da97e"
            ],
            "layout": "IPY_MODEL_d5d01a50802546a9bc400b80898b2622"
          }
        },
        "c6f18fed2c69415daf8f8afbdf6cfa53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_782c469d077144f584a857350527a95a",
            "placeholder": "​",
            "style": "IPY_MODEL_40d6e8d4d6684f93b3db8e6d9300dd82",
            "value": "model.safetensors: 100%"
          }
        },
        "7ba9714025134ccd94a61fce4b0efbc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cd970a882e9473ea71b5f241d94ee71",
            "max": 605157884,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5d816bd0f8a4ebf9dc1082ba7246fe0",
            "value": 605157884
          }
        },
        "66aa98e9d960434396eb196db47da97e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b280c4f7825e4024b472817df71d4084",
            "placeholder": "​",
            "style": "IPY_MODEL_fd38a83037b54d958cba149b54dff514",
            "value": " 605M/605M [00:02&lt;00:00, 371MB/s]"
          }
        },
        "d5d01a50802546a9bc400b80898b2622": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "782c469d077144f584a857350527a95a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40d6e8d4d6684f93b3db8e6d9300dd82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cd970a882e9473ea71b5f241d94ee71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5d816bd0f8a4ebf9dc1082ba7246fe0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b280c4f7825e4024b472817df71d4084": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd38a83037b54d958cba149b54dff514": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ossnat/VSD_foundation_model_evaluation/blob/main/LoopOverPretrainedModels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qCMkO4eJpAX",
        "outputId": "dde810db-0bae-4247-ceed-6a85a85f077d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Gonda_research_Hamutal_2024/Ossnat/VSD_FM_evaluate/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMqTWObgLQEX",
        "outputId": "2b925add-37c0-426b-904b-6c2efe870a24"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Gonda_research_Hamutal_2024/Ossnat/VSD_FM_evaluate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "63ELjhm6QBxI",
        "outputId": "6f0c1cfc-ae1e-4ec6-c141-836fa2c3924b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Gonda_research_Hamutal_2024/Ossnat/VSD_FM_evaluate'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# if not os.path.exists(\"VSD_foundation_model_evaluation\"):\n",
        "#     !git clone https://github.com/ossnat/VSD_foundation_model_evaluation.git\n"
      ],
      "metadata": {
        "id": "4iVQmjsaLILu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd VSD_foundation_model_evaluation/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsluZQj-MD8l",
        "outputId": "c201496e-270c-4f31-a2af-c77edf2034c7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Gonda_research_Hamutal_2024/Ossnat/VSD_FM_evaluate/VSD_foundation_model_evaluation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deY--lqsRWqv",
        "outputId": "64c887f7-5efa-428c-a1c8-52436bb00330"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects:   5% (1/18)\u001b[K\rremote: Counting objects:  11% (2/18)\u001b[K\rremote: Counting objects:  16% (3/18)\u001b[K\rremote: Counting objects:  22% (4/18)\u001b[K\rremote: Counting objects:  27% (5/18)\u001b[K\rremote: Counting objects:  33% (6/18)\u001b[K\rremote: Counting objects:  38% (7/18)\u001b[K\rremote: Counting objects:  44% (8/18)\u001b[K\rremote: Counting objects:  50% (9/18)\u001b[K\rremote: Counting objects:  55% (10/18)\u001b[K\rremote: Counting objects:  61% (11/18)\u001b[K\rremote: Counting objects:  66% (12/18)\u001b[K\rremote: Counting objects:  72% (13/18)\u001b[K\rremote: Counting objects:  77% (14/18)\u001b[K\rremote: Counting objects:  83% (15/18)\u001b[K\rremote: Counting objects:  88% (16/18)\u001b[K\rremote: Counting objects:  94% (17/18)\u001b[K\rremote: Counting objects: 100% (18/18)\u001b[K\rremote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects:   9% (1/11)\u001b[K\rremote: Compressing objects:  18% (2/11)\u001b[K\rremote: Compressing objects:  27% (3/11)\u001b[K\rremote: Compressing objects:  36% (4/11)\u001b[K\rremote: Compressing objects:  45% (5/11)\u001b[K\rremote: Compressing objects:  54% (6/11)\u001b[K\rremote: Compressing objects:  63% (7/11)\u001b[K\rremote: Compressing objects:  72% (8/11)\u001b[K\rremote: Compressing objects:  81% (9/11)\u001b[K\rremote: Compressing objects:  90% (10/11)\u001b[K\rremote: Compressing objects: 100% (11/11)\u001b[K\rremote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 14 (delta 7), reused 10 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (14/14), 4.37 KiB | 3.00 KiB/s, done.\n",
            "From https://github.com/ossnat/VSD_foundation_model_evaluation\n",
            "   8da4a84..1555362  main       -> origin/main\n",
            "Updating 8da4a84..1555362\n",
            "Fast-forward\n",
            " models/backbones.py        |  76 \u001b[32m++++++++++++\u001b[m\u001b[31m------\u001b[m\n",
            " models/model_evaluation.py | 187 \u001b[32m+++++++++++++++++++++++++++++++++++++++++++++\u001b[m\n",
            " 2 files changed, 239 insertions(+), 24 deletions(-)\n",
            " create mode 100644 models/model_evaluation.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Gonda_research_Hamutal_2024/Ossnat/VSD_FM_evaluate/VSD_foundation_model_evaluation')\n",
        "sys.path.append('/content/drive/MyDrive/Gonda_research_Hamutal_2024/Ossnat/VSD_FM_evaluate/VSD_foundation_model_evaluation/src')\n",
        "sys.path.append('/content/drive/MyDrive/Gonda_research_Hamutal_2024/Ossnat/VSD_FM_evaluate/VSD_foundation_model_evaluation/models')\n",
        "sys.path.append('/content/drive/MyDrive/Gonda_research_Hamutal_2024/Ossnat/VSD_FM_evaluate/VSD_foundation_model_evaluation/config')\n",
        "sys.path.append('/content/drive/MyDrive/Gonda_research_Hamutal_2024/Ossnat/VSD_FM_evaluate/VSD_foundation_model_evaluation/data_utils')"
      ],
      "metadata": {
        "id": "hYAOwFfRMq0O"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "import os\n",
        "\n",
        "# Construct the full path to the config file\n",
        "config_path = os.path.join(\"config\", \"model_config.yaml\")\n",
        "\n",
        "with open(config_path, \"r\") as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Ensure num_classes is set to 2 for binary classification\n",
        "config[\"model\"][\"num_classes\"] = 2\n",
        "print(\"Loaded config with num_classes set to 2:\")\n",
        "print(config)"
      ],
      "metadata": {
        "id": "n4Wsi-F5V5f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "699c654a-3cbf-4f15-fd51-01ab0de84088"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded config with num_classes set to 2:\n",
            "{'training': {'epochs': 20, 'batch_size': 8, 'learning_rate': 0.001, 'device': 'auto'}, 'data': {'dataset_path': './data/frodo_early', 'num_frames': 5, 'num_workers': 4, 'start_frame': 28, 'end_frame': 58}, 'model': {'backbone': 'dino', 'temporal_pooling': 'mean', 'embedding_dim': 768, 'num_classes': 2}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load Data"
      ],
      "metadata": {
        "id": "ZSxc3fnmX0NM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import data_utils.prepare_data\n",
        "import src"
      ],
      "metadata": {
        "id": "8mujWm1FTzfC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from data_utils.prepare_data import prepare_vsd_data, preprocess_vsd_clip, get_train_val_test_loaders\n",
        "from src.dataset import VSDClipsDataset, split_trials\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "Aekr979d2gpv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "vertical_path = \"./data/frodo_early_0/vertical_data.npy\"\n",
        "horizontal_path = \"./data/frodo_early_0//horizontal_data.npy\"\n",
        "\n",
        "vertical_data = prepare_vsd_data(vertical_path)     # (117, 80, 1, 100, 100)\n",
        "horizontal_data = prepare_vsd_data(horizontal_path)\n",
        "\n",
        "# Build labels: 0 for vertical, 1 for horizontal\n",
        "vertical_labels = np.zeros(len(vertical_data), dtype=int)\n",
        "horizontal_labels = np.ones(len(horizontal_data), dtype=int)\n",
        "\n",
        "# Concatenate\n",
        "data = np.concatenate([vertical_data, horizontal_data], axis=0)\n",
        "original_labels = np.concatenate([vertical_labels, horizontal_labels], axis=0)\n",
        "\n",
        "labels = original_labels.copy()\n",
        "print(\"Final data shape:\", data.shape)   # (N, 80, 1, 100, 100)\n",
        "print(\"Labels shape:\", labels.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4hp0SY-2qkJ",
        "outputId": "19e27274-c30a-4c5f-dbaf-4d9baaaadcc7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final data shape: (62, 80, 1, 100, 100)\n",
            "Labels shape: (62,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "train_idx, val_idx, test_idx = split_trials(data, labels)\n",
        "\n",
        "# Build datasets\n",
        "train_dataset = VSDClipsDataset(data[train_idx], labels[train_idx], config, clip_len=5)\n",
        "val_dataset   = VSDClipsDataset(data[val_idx], labels[val_idx], config, clip_len=5)\n",
        "test_dataset  = VSDClipsDataset(data[test_idx], labels[test_idx], config, clip_len=5)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0) #!!! FIX back!!!\n",
        "val_loader   = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n"
      ],
      "metadata": {
        "id": "jDd-L_c17RxJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, val_loader, test_loader = get_train_val_test_loaders(data, labels, VSDClipsDataset, DataLoader, config,\n",
        "                               train_idx, val_idx, test_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fN2nB0iL7tnX",
        "outputId": "009e7882-c026-42cd-c296-e6ebce3e9665"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "get_train_val_test_loaders: Shape of data and labels: (62, 80, 1, 100, 100), (62,)\n",
            "get_train_val_test_loaders: Length of train_idx: 43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get one batch from the dataloader\n",
        "clips, batch_labels = next(iter(train_loader))\n",
        "\n",
        "print('### clip shape:',clips.shape, type(clips))\n",
        "print('### batch_labels shape:', batch_labels.shape, type(batch_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTnIRrjg9WyI",
        "outputId": "cf9396b1-9b24-48cb-d0e2-211a807e431d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### clip shape: torch.Size([8, 5, 3, 224, 224]) <class 'torch.Tensor'>\n",
            "### batch_labels shape: torch.Size([8]) <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Get one batch from the dataloader\n",
        "clips, batch_labels = next(iter(train_loader))\n",
        "# clips shape: (B, T, C, H, W) -> (8, 5, 1, 100, 100)\n",
        "print('### clips shape:',clips.shape, type(clips))\n",
        "\n",
        "# Pick the first clip in the batch\n",
        "clip = clips[0]  # shape: (T, C, H, W) -> (5, 1, 100, 100)\n",
        "print('### clip shape:',clip.shape, type(clip))\n",
        "label = batch_labels[0].item() # Use batch_labels here\n",
        "\n",
        "# Pick 3 random frame indices\n",
        "T = clip.shape[0]\n",
        "frame_indices = random.sample(range(T), 3) # Select 3 random indices\n",
        "\n",
        "# Plot frames\n",
        "plt.figure(figsize=(12, 4))\n",
        "for i, idx in enumerate(frame_indices):\n",
        "    frame = clip[idx].squeeze().cpu().numpy()  # (H, W) after squeezing C=1\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    plt.imshow(frame[1,:,:], cmap=\"gray\")\n",
        "    plt.title(f\"Frame {idx}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.suptitle(f\"Clip from Trial with Label: {label}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "FXx_fYJl_Djf",
        "outputId": "b1cf849f-64ea-414e-b3bf-2ccff32e873a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### clips shape: torch.Size([8, 5, 3, 224, 224]) <class 'torch.Tensor'>\n",
            "### clip shape: torch.Size([5, 3, 224, 224]) <class 'torch.Tensor'>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Labels quick check\n",
        "from collections import Counter\n",
        "all_labels_sampled = [] # Use a different variable name to avoid conflict\n",
        "for _, batch_labels in train_loader: # Use batch_labels here\n",
        "    all_labels_sampled.extend(batch_labels.numpy().tolist()) # Use batch_labels here\n",
        "    if len(all_labels_sampled) > 1000: break\n",
        "print(\"label counts (sampled):\", Counter(all_labels_sampled))\n",
        "\n",
        "# print first batch labels\n",
        "clips, batch_labels = next(iter(train_loader)) # Use batch_labels here\n",
        "print(\"batch labels:\", batch_labels) # Use batch_labels here"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xXZlKL-fnrr",
        "outputId": "4cd86abb-36b8-4711-87b2-e781a2a7599b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label counts (sampled): Counter({1: 518, 0: 490})\n",
            "batch labels: tensor([0, 0, 0, 0, 1, 1, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "clips, labels = next(iter(train_loader))  # clips: (B,T,C,H,W)\n",
        "clip = clips[0]   # first clip\n",
        "label = labels[0].item()\n",
        "T = clip.shape[0]\n",
        "plt.figure(figsize=(12,2))\n",
        "for i in range(min(T,5)):\n",
        "    im = clip[i].permute(1,2,0).squeeze().cpu().numpy()\n",
        "    plt.subplot(1,5,i+1); plt.imshow(im, cmap='gray'); plt.title(f\"f{i}\");\n",
        "plt.suptitle(f\"label={label}\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "3Eh12f8IgJaf",
        "outputId": "03577273-3ffb-49b3-8616-d38ad68ce249"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.86505055..14.887158].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.327964..18.727417].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.88202244..15.062716].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.5452937..16.782284].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.3500443..16.382786].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x200 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train model"
      ],
      "metadata": {
        "id": "msu-_7roFekl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "utHMnrAbKc-e"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0Mzl-piZJfqU"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 8\n",
        "EPOCHS = 5\n",
        "LR = 1e-4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding_dim(config):\n",
        "    backbone_name = config[\"model\"][\"backbone\"]\n",
        "    if 'dino' in backbone_name:\n",
        "        return 768\n",
        "    elif 'resnet' in backbone_name:\n",
        "        return 512\n",
        "    else:\n",
        "      print(f\"Warning: Unknown backbone '{backbone_name}', using default embedding_dim={embedding_dim}\")\n",
        "      return config[\"model\"].get(\"embedding_dim\", 768)\n"
      ],
      "metadata": {
        "id": "3Q-ClSxPC4QH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.classifier import VideoClassifier\n",
        "from src.train import train_model\n",
        "\n",
        "# for debug only -- REMOVE!!!!\n",
        "config[\"training\"][\"epochs\"] = 5\n",
        "config[\"model\"][\"embedding_dim\"] = get_embedding_dim(config)"
      ],
      "metadata": {
        "id": "mIzDts5WRFP2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick Sanity Test:\n",
        "clips, labels = next(iter(train_loader))  # [B, T, C, H, W]\n",
        "B, T, C, H, W = clips.shape\n",
        "clips_flat = clips.view(B*T, C, H, W)\n",
        "print(\"Flattened clips shape:\", clips_flat.shape, flush=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrN1j0dhlpfd",
        "outputId": "c755d19f-f5be-4225-a01f-d60c21e51b5a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flattened clips shape: torch.Size([40, 3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a45bb090"
      },
      "source": [
        "## Set up directories\n",
        "\n",
        "### Subtask:\n",
        "Create directories for saving trained models (`trained_models`) and evaluation results (`test_results`) if they don't exist.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d663f38a"
      },
      "source": [
        "backbones = ['dino2d', 'resnet18']\n",
        "temporal_pooling_methods = ['mean', 'attention']"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb1c710a",
        "outputId": "5ce96ac9-14d8-4722-8a81-fca716125d2f"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define directory paths\n",
        "trained_models_dir = \"trained_models\"\n",
        "test_results_dir = \"test_results\"\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(trained_models_dir, exist_ok=True)\n",
        "os.makedirs(test_results_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Created directory: {trained_models_dir}\")\n",
        "print(f\"Created directory: {test_results_dir}\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created directory: trained_models\n",
            "Created directory: test_results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "189e0d62"
      },
      "source": [
        "### Training and evaluation loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47696e32",
        "outputId": "45b9f5eb-c2aa-4f0e-f68b-f32501e70611"
      },
      "source": [
        "for backbone_name in backbones:\n",
        "    for temporal_pooling_method in temporal_pooling_methods:\n",
        "        print(f\"Processing backbone: {backbone_name}, temporal pooling: {temporal_pooling_method}\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing backbone: dino2d, temporal pooling: mean\n",
            "Processing backbone: dino2d, temporal pooling: attention\n",
            "Processing backbone: resnet18, temporal pooling: mean\n",
            "Processing backbone: resnet18, temporal pooling: attention\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = config[\"training\"][\"device\"]\n",
        "print('#######', device)\n",
        "if device == \"auto\":\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "else:\n",
        "  device = torch.device(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9AT2WrqjI19",
        "outputId": "499632fc-1553-47e2-eb81-eab9c7e7acc0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "####### auto\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3a011b9f96a3460ba3cf3f734f88d04f",
            "24205e095ea14292b60a8775688fff9a",
            "a36562b7bb084d608b1822d213ccbf75",
            "105dfd7b796b4a4d8628f2549ce33126",
            "4f9843756bbb47e1b1d93e5d607957ae",
            "d17e4b9871c14533928beda978182e8b",
            "51ec61dd784b4cdca680b4fbf98292ff",
            "e56fddfb202b4ca2bb553f3cf59b25de",
            "2b4cbdc1ee0f4bf09a644d215284900b",
            "4db14d6c8dc0410ebcc99755616e860d",
            "edc6ae75d94a455991a6596f8f1cb0cc",
            "6d6b7e842f0d440eb80dab462f6f46f5",
            "758ca77b22cc4486bada6eb124173ab2",
            "f3b3c39c5c924ecb8ccd5f2e9dcdf48b",
            "85a1640a9fe44e89b6110a3f03b30736",
            "cc4310128a584317a40b7d35be83e042",
            "5c0a1ed525194d2cab575e6889a44139",
            "9168cfd6408243949fdcd40e21aeae7d",
            "676a748a2e0443239cf3e4b336e04823",
            "8ab380126e4649458d32018c2483b118",
            "7ea0d56aa7154e989d8bc6547f9711df",
            "7749e3701a6544b095d6662c45350bec",
            "3ef19ceb9f494d67bc27a63db634ef82",
            "c6f18fed2c69415daf8f8afbdf6cfa53",
            "7ba9714025134ccd94a61fce4b0efbc0",
            "66aa98e9d960434396eb196db47da97e",
            "d5d01a50802546a9bc400b80898b2622",
            "782c469d077144f584a857350527a95a",
            "40d6e8d4d6684f93b3db8e6d9300dd82",
            "9cd970a882e9473ea71b5f241d94ee71",
            "b5d816bd0f8a4ebf9dc1082ba7246fe0",
            "b280c4f7825e4024b472817df71d4084",
            "fd38a83037b54d958cba149b54dff514"
          ]
        },
        "id": "2f25150d",
        "outputId": "332e8fd8-f627-4d1a-eaf9-f8740811d0ff"
      },
      "source": [
        "for backbone_name in backbones:\n",
        "    for temporal_pooling_method in temporal_pooling_methods:\n",
        "        print(f\"\\n--- Processing backbone: {backbone_name}, temporal pooling: {temporal_pooling_method} ---\")\n",
        "\n",
        "        # Construct unique filename and model path\n",
        "        n_epochs = config[\"training\"][\"epochs\"]\n",
        "        model_filename = f\"{backbone_name}_{temporal_pooling_method}_epochs_{n_epochs}_model.pth\"\n",
        "        model_path = os.path.join(trained_models_dir, model_filename)\n",
        "\n",
        "        # update config\n",
        "        config[\"model\"][\"backbone\"] = backbone_name\n",
        "        config[\"model\"][\"temporal_pooling\"] = temporal_pooling_method\n",
        "        config[\"model\"][\"embedding_dim\"] = get_embedding_dim(config)\n",
        "        labels = original_labels.copy()\n",
        "\n",
        "        # Add print statements for debugging\n",
        "        print(f\"Shape of data before calling get_train_val_test_loaders: {data.shape}\")\n",
        "        print(f\"Length of train_idx: {len(train_idx)}\")\n",
        "        # print(f\"First 5 elements of train_idx: {train_idx[:5]}\")\n",
        "\n",
        "\n",
        "        train_loader, val_loader, test_loader = get_train_val_test_loaders(data, labels, VSDClipsDataset, DataLoader, config,\n",
        "                               train_idx, val_idx, test_idx)\n",
        "\n",
        "        if os.path.exists(model_path):\n",
        "            print(f\"Loading pre-trained model from {model_path}\")\n",
        "            # Need to initialize model structure before loading state_dict\n",
        "            model = VideoClassifier(\n",
        "                 backbone_name=config[\"model\"][\"backbone\"],\n",
        "                 temporal_pooling=config[\"model\"][\"temporal_pooling\"],\n",
        "                 embedding_dim=config[\"model\"][\"embedding_dim\"],\n",
        "                 num_classes=config[\"model\"][\"num_classes\"],\n",
        "             ).to(device) # Assuming 'device' is accessible\n",
        "            model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "            # Since we loaded, history is not available from training\n",
        "            history = None\n",
        "            trained_model = model # Set trained_model to the loaded model\n",
        "        else:\n",
        "            # Need to define or access 'device' here.\n",
        "            # Since train_model returns device, we should call train_model first\n",
        "            # or ensure device is determined before the loop.\n",
        "            # Let's assume device is determined before the loop.\n",
        "            print(f\"Training new model for backbone: {backbone_name}, temporal pooling: {temporal_pooling_method}\")\n",
        "            history, trained_model, device = train_model(config, train_loader, val_loader)\n",
        "\n",
        "            torch.save(trained_model.state_dict(), model_path)\n",
        "            print(f\"Model saved to {model_path}\")\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Processing backbone: dino2d, temporal pooling: mean ---\n",
            "Shape of data before calling get_train_val_test_loaders: (62, 80, 1, 100, 100)\n",
            "Length of train_idx: 43\n",
            "get_train_val_test_loaders: Shape of data and labels: (62, 80, 1, 100, 100), (62,)\n",
            "get_train_val_test_loaders: Length of train_idx: 43\n",
            "Training new model for backbone: dino2d, temporal pooling: mean\n",
            "####### auto\n",
            "--- build_backbone -- dino2d\n",
            "--- build_backbone model name:  openai/clip-vit-base-patch32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a011b9f96a3460ba3cf3f734f88d04f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d6b7e842f0d440eb80dab462f6f46f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   0%|          | 0/140 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ef19ceb9f494d67bc27a63db634ef82"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   1%|          | 1/140 [00:03<08:41,  3.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   1%|▏         | 2/140 [00:04<03:58,  1.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   2%|▏         | 3/140 [00:04<02:28,  1.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   3%|▎         | 4/140 [00:04<01:46,  1.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   4%|▎         | 5/140 [00:05<01:23,  1.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   4%|▍         | 6/140 [00:05<01:09,  1.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   5%|▌         | 7/140 [00:05<01:00,  2.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   6%|▌         | 8/140 [00:05<00:54,  2.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   6%|▋         | 9/140 [00:06<00:50,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   7%|▋         | 10/140 [00:06<00:47,  2.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   8%|▊         | 11/140 [00:06<00:45,  2.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   9%|▊         | 12/140 [00:07<00:43,  2.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   9%|▉         | 13/140 [00:07<00:42,  2.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  10%|█         | 14/140 [00:07<00:41,  3.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  11%|█         | 15/140 [00:08<00:41,  3.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  11%|█▏        | 16/140 [00:08<00:40,  3.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  12%|█▏        | 17/140 [00:08<00:40,  3.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  13%|█▎        | 18/140 [00:09<00:39,  3.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  14%|█▎        | 19/140 [00:09<00:39,  3.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  14%|█▍        | 20/140 [00:09<00:38,  3.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  15%|█▌        | 21/140 [00:10<00:38,  3.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  16%|█▌        | 22/140 [00:10<00:37,  3.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  16%|█▋        | 23/140 [00:10<00:37,  3.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  17%|█▋        | 24/140 [00:11<00:37,  3.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  18%|█▊        | 25/140 [00:11<00:36,  3.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  19%|█▊        | 26/140 [00:11<00:36,  3.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  19%|█▉        | 27/140 [00:12<00:36,  3.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  20%|██        | 28/140 [00:12<00:36,  3.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  21%|██        | 29/140 [00:12<00:35,  3.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  21%|██▏       | 30/140 [00:13<00:35,  3.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  22%|██▏       | 31/140 [00:13<00:35,  3.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  23%|██▎       | 32/140 [00:13<00:35,  3.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  24%|██▎       | 33/140 [00:14<00:34,  3.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  24%|██▍       | 34/140 [00:14<00:34,  3.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  25%|██▌       | 35/140 [00:14<00:34,  3.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  26%|██▌       | 36/140 [00:15<00:33,  3.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  26%|██▋       | 37/140 [00:15<00:33,  3.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  27%|██▋       | 38/140 [00:15<00:33,  3.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  28%|██▊       | 39/140 [00:16<00:32,  3.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  29%|██▊       | 40/140 [00:16<00:32,  3.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  29%|██▉       | 41/140 [00:16<00:32,  3.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  30%|███       | 42/140 [00:16<00:31,  3.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  31%|███       | 43/140 [00:17<00:31,  3.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  31%|███▏      | 44/140 [00:17<00:31,  3.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  32%|███▏      | 45/140 [00:17<00:30,  3.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  33%|███▎      | 46/140 [00:18<00:30,  3.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  34%|███▎      | 47/140 [00:18<00:30,  3.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  34%|███▍      | 48/140 [00:18<00:29,  3.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  35%|███▌      | 49/140 [00:19<00:29,  3.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  36%|███▌      | 50/140 [00:19<00:29,  3.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  36%|███▋      | 51/140 [00:19<00:28,  3.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  37%|███▋      | 52/140 [00:20<00:28,  3.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  38%|███▊      | 53/140 [00:20<00:28,  3.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  39%|███▊      | 54/140 [00:20<00:28,  3.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  39%|███▉      | 55/140 [00:21<00:27,  3.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  40%|████      | 56/140 [00:21<00:27,  3.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  41%|████      | 57/140 [00:21<00:27,  3.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  41%|████▏     | 58/140 [00:22<00:26,  3.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  42%|████▏     | 59/140 [00:22<00:26,  3.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  43%|████▎     | 60/140 [00:22<00:26,  3.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  44%|████▎     | 61/140 [00:23<00:25,  3.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  44%|████▍     | 62/140 [00:23<00:25,  3.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  45%|████▌     | 63/140 [00:23<00:25,  3.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  46%|████▌     | 64/140 [00:24<00:24,  3.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  46%|████▋     | 65/140 [00:24<00:24,  3.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  47%|████▋     | 66/140 [00:24<00:24,  3.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  48%|████▊     | 67/140 [00:25<00:24,  3.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  49%|████▊     | 68/140 [00:25<00:23,  3.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  49%|████▉     | 69/140 [00:25<00:23,  3.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  50%|█████     | 70/140 [00:26<00:23,  3.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  51%|█████     | 71/140 [00:26<00:22,  3.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  51%|█████▏    | 72/140 [00:26<00:22,  3.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  52%|█████▏    | 73/140 [00:27<00:22,  3.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  53%|█████▎    | 74/140 [00:27<00:21,  3.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  54%|█████▎    | 75/140 [00:27<00:21,  3.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  54%|█████▍    | 76/140 [00:28<00:21,  3.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  55%|█████▌    | 77/140 [00:28<00:20,  3.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  56%|█████▌    | 78/140 [00:28<00:20,  3.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  56%|█████▋    | 79/140 [00:29<00:20,  3.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  57%|█████▋    | 80/140 [00:29<00:19,  3.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  58%|█████▊    | 81/140 [00:29<00:19,  3.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  59%|█████▊    | 82/140 [00:30<00:19,  2.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  59%|█████▉    | 83/140 [00:30<00:19,  2.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  60%|██████    | 84/140 [00:30<00:18,  3.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  61%|██████    | 85/140 [00:31<00:18,  3.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  61%|██████▏   | 86/140 [00:31<00:18,  3.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  62%|██████▏   | 87/140 [00:31<00:17,  2.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  63%|██████▎   | 88/140 [00:32<00:17,  2.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  64%|██████▎   | 89/140 [00:32<00:16,  3.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  64%|██████▍   | 90/140 [00:32<00:16,  3.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  65%|██████▌   | 91/140 [00:33<00:16,  3.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  66%|██████▌   | 92/140 [00:33<00:15,  3.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  66%|██████▋   | 93/140 [00:33<00:15,  3.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  67%|██████▋   | 94/140 [00:34<00:15,  3.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  68%|██████▊   | 95/140 [00:34<00:14,  3.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  69%|██████▊   | 96/140 [00:34<00:14,  3.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  69%|██████▉   | 97/140 [00:35<00:14,  2.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  70%|███████   | 98/140 [00:35<00:13,  3.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  71%|███████   | 99/140 [00:35<00:13,  3.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  71%|███████▏  | 100/140 [00:36<00:13,  3.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  72%|███████▏  | 101/140 [00:36<00:13,  3.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  73%|███████▎  | 102/140 [00:36<00:12,  3.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  74%|███████▎  | 103/140 [00:37<00:12,  3.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  74%|███████▍  | 104/140 [00:37<00:12,  2.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  75%|███████▌  | 105/140 [00:37<00:11,  2.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  76%|███████▌  | 106/140 [00:38<00:11,  2.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  76%|███████▋  | 107/140 [00:38<00:11,  2.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  77%|███████▋  | 108/140 [00:38<00:10,  2.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  78%|███████▊  | 109/140 [00:39<00:10,  2.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  79%|███████▊  | 110/140 [00:39<00:10,  2.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  79%|███████▉  | 111/140 [00:39<00:09,  2.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  80%|████████  | 112/140 [00:40<00:09,  2.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  81%|████████  | 113/140 [00:40<00:09,  2.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  81%|████████▏ | 114/140 [00:40<00:08,  2.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  82%|████████▏ | 115/140 [00:41<00:08,  2.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  83%|████████▎ | 116/140 [00:41<00:08,  2.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  84%|████████▎ | 117/140 [00:41<00:07,  2.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  84%|████████▍ | 118/140 [00:42<00:07,  2.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  85%|████████▌ | 119/140 [00:42<00:07,  2.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  86%|████████▌ | 120/140 [00:42<00:06,  2.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  86%|████████▋ | 121/140 [00:43<00:06,  2.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  87%|████████▋ | 122/140 [00:43<00:06,  2.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  88%|████████▊ | 123/140 [00:43<00:05,  2.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  89%|████████▊ | 124/140 [00:44<00:05,  2.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  89%|████████▉ | 125/140 [00:44<00:05,  2.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  90%|█████████ | 126/140 [00:44<00:04,  2.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  91%|█████████ | 127/140 [00:45<00:04,  2.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  91%|█████████▏| 128/140 [00:45<00:04,  2.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  92%|█████████▏| 129/140 [00:45<00:03,  2.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  93%|█████████▎| 130/140 [00:46<00:03,  2.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  94%|█████████▎| 131/140 [00:46<00:03,  2.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  94%|█████████▍| 132/140 [00:46<00:02,  2.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  95%|█████████▌| 133/140 [00:47<00:02,  2.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  96%|█████████▌| 134/140 [00:47<00:02,  2.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  96%|█████████▋| 135/140 [00:47<00:01,  2.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  97%|█████████▋| 136/140 [00:48<00:01,  2.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  98%|█████████▊| 137/140 [00:48<00:01,  2.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  99%|█████████▊| 138/140 [00:48<00:00,  2.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  99%|█████████▉| 139/140 [00:49<00:00,  2.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([30, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([30, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([30, 768])\n",
            "Final output shape: torch.Size([30, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 140/140 [00:49<00:00,  2.82it/s]\n",
            "Valid:   3%|▎         | 1/30 [00:00<00:07,  3.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  10%|█         | 3/30 [00:00<00:04,  6.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  17%|█▋        | 5/30 [00:00<00:03,  7.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  23%|██▎       | 7/30 [00:00<00:02,  8.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  30%|███       | 9/30 [00:01<00:02,  8.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  37%|███▋      | 11/30 [00:01<00:02,  8.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  43%|████▎     | 13/30 [00:01<00:01,  8.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  50%|█████     | 15/30 [00:01<00:01,  8.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  57%|█████▋    | 17/30 [00:02<00:01,  8.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  63%|██████▎   | 19/30 [00:02<00:01,  8.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  70%|███████   | 21/30 [00:02<00:01,  8.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  77%|███████▋  | 23/30 [00:02<00:00,  8.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  83%|████████▎ | 25/30 [00:03<00:00,  8.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  90%|█████████ | 27/30 [00:03<00:00,  8.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 30/30 [00:03<00:00,  8.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([10, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([10, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([10, 768])\n",
            "Final output shape: torch.Size([10, 768])\n",
            "Epoch 1: train_acc=0.511, val_acc=0.457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   0%|          | 0/140 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   1%|          | 1/140 [00:00<01:05,  2.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   1%|▏         | 2/140 [00:00<00:53,  2.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   2%|▏         | 3/140 [00:01<00:51,  2.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   3%|▎         | 4/140 [00:01<00:49,  2.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   4%|▎         | 5/140 [00:01<00:47,  2.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   4%|▍         | 6/140 [00:02<00:47,  2.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   5%|▌         | 7/140 [00:02<00:46,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   6%|▌         | 8/140 [00:02<00:45,  2.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   6%|▋         | 9/140 [00:03<00:45,  2.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   7%|▋         | 10/140 [00:03<00:44,  2.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   8%|▊         | 11/140 [00:03<00:44,  2.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   9%|▊         | 12/140 [00:04<00:43,  2.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   9%|▉         | 13/140 [00:04<00:43,  2.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  10%|█         | 14/140 [00:04<00:43,  2.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  11%|█         | 15/140 [00:05<00:42,  2.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  11%|█▏        | 16/140 [00:05<00:42,  2.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  12%|█▏        | 17/140 [00:05<00:42,  2.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  13%|█▎        | 18/140 [00:06<00:41,  2.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  14%|█▎        | 19/140 [00:06<00:41,  2.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  14%|█▍        | 20/140 [00:06<00:40,  2.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  15%|█▌        | 21/140 [00:07<00:40,  2.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  16%|█▌        | 22/140 [00:07<00:40,  2.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  16%|█▋        | 23/140 [00:07<00:39,  2.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  17%|█▋        | 24/140 [00:08<00:39,  2.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  18%|█▊        | 25/140 [00:08<00:39,  2.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  19%|█▊        | 26/140 [00:09<00:39,  2.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  19%|█▉        | 27/140 [00:09<00:38,  2.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  20%|██        | 28/140 [00:09<00:38,  2.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  21%|██        | 29/140 [00:10<00:37,  2.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  21%|██▏       | 30/140 [00:10<00:37,  2.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  22%|██▏       | 31/140 [00:10<00:37,  2.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  23%|██▎       | 32/140 [00:11<00:36,  2.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  24%|██▎       | 33/140 [00:11<00:36,  2.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  24%|██▍       | 34/140 [00:11<00:36,  2.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  25%|██▌       | 35/140 [00:12<00:35,  2.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  26%|██▌       | 36/140 [00:12<00:35,  2.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  26%|██▋       | 37/140 [00:12<00:35,  2.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  27%|██▋       | 38/140 [00:13<00:34,  2.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  28%|██▊       | 39/140 [00:13<00:34,  2.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  29%|██▊       | 40/140 [00:13<00:34,  2.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  29%|██▉       | 41/140 [00:14<00:33,  2.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  30%|███       | 42/140 [00:14<00:33,  2.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  31%|███       | 43/140 [00:14<00:33,  2.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  31%|███▏      | 44/140 [00:15<00:32,  2.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  32%|███▏      | 45/140 [00:15<00:32,  2.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  33%|███▎      | 46/140 [00:15<00:32,  2.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  34%|███▎      | 47/140 [00:16<00:31,  2.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  34%|███▍      | 48/140 [00:16<00:31,  2.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  35%|███▌      | 49/140 [00:16<00:31,  2.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  36%|███▌      | 50/140 [00:17<00:30,  2.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  36%|███▋      | 51/140 [00:17<00:30,  2.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  37%|███▋      | 52/140 [00:17<00:30,  2.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  38%|███▊      | 53/140 [00:18<00:29,  2.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  39%|███▊      | 54/140 [00:18<00:29,  2.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  39%|███▉      | 55/140 [00:18<00:29,  2.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  40%|████      | 56/140 [00:19<00:28,  2.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  41%|████      | 57/140 [00:19<00:28,  2.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  41%|████▏     | 58/140 [00:20<00:28,  2.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  42%|████▏     | 59/140 [00:20<00:28,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  43%|████▎     | 60/140 [00:20<00:27,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  44%|████▎     | 61/140 [00:21<00:27,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  44%|████▍     | 62/140 [00:21<00:27,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  45%|████▌     | 63/140 [00:21<00:26,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  46%|████▌     | 64/140 [00:22<00:26,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  46%|████▋     | 65/140 [00:22<00:25,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  47%|████▋     | 66/140 [00:22<00:25,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  48%|████▊     | 67/140 [00:23<00:25,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  49%|████▊     | 68/140 [00:23<00:25,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  49%|████▉     | 69/140 [00:23<00:24,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  50%|█████     | 70/140 [00:24<00:24,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  51%|█████     | 71/140 [00:24<00:23,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  51%|█████▏    | 72/140 [00:24<00:23,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  52%|█████▏    | 73/140 [00:25<00:23,  2.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  53%|█████▎    | 74/140 [00:25<00:22,  2.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  54%|█████▎    | 75/140 [00:25<00:22,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  54%|█████▍    | 76/140 [00:26<00:22,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  55%|█████▌    | 77/140 [00:26<00:21,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  56%|█████▌    | 78/140 [00:26<00:21,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  56%|█████▋    | 79/140 [00:27<00:21,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  57%|█████▋    | 80/140 [00:27<00:20,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  58%|█████▊    | 81/140 [00:27<00:20,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  59%|█████▊    | 82/140 [00:28<00:20,  2.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  59%|█████▉    | 83/140 [00:28<00:19,  2.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  60%|██████    | 84/140 [00:29<00:19,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  61%|██████    | 85/140 [00:29<00:19,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  61%|██████▏   | 86/140 [00:29<00:18,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  62%|██████▏   | 87/140 [00:30<00:18,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  63%|██████▎   | 88/140 [00:30<00:18,  2.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  64%|██████▎   | 89/140 [00:30<00:17,  2.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  64%|██████▍   | 90/140 [00:31<00:17,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  65%|██████▌   | 91/140 [00:31<00:17,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  66%|██████▌   | 92/140 [00:31<00:16,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  66%|██████▋   | 93/140 [00:32<00:16,  2.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  67%|██████▋   | 94/140 [00:32<00:15,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  68%|██████▊   | 95/140 [00:32<00:15,  2.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  69%|██████▊   | 96/140 [00:33<00:15,  2.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  69%|██████▉   | 97/140 [00:33<00:14,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  70%|███████   | 98/140 [00:33<00:14,  2.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  71%|███████   | 99/140 [00:34<00:14,  2.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  71%|███████▏  | 100/140 [00:34<00:13,  2.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  72%|███████▏  | 101/140 [00:34<00:13,  2.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  73%|███████▎  | 102/140 [00:35<00:13,  2.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  74%|███████▎  | 103/140 [00:35<00:12,  2.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  74%|███████▍  | 104/140 [00:35<00:12,  2.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  75%|███████▌  | 105/140 [00:36<00:12,  2.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  76%|███████▌  | 106/140 [00:36<00:11,  2.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  76%|███████▋  | 107/140 [00:37<00:11,  2.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  77%|███████▋  | 108/140 [00:37<00:11,  2.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  78%|███████▊  | 109/140 [00:37<00:10,  2.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  79%|███████▊  | 110/140 [00:38<00:10,  2.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  79%|███████▉  | 111/140 [00:38<00:10,  2.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  80%|████████  | 112/140 [00:38<00:09,  2.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  81%|████████  | 113/140 [00:39<00:09,  2.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  81%|████████▏ | 114/140 [00:39<00:09,  2.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  82%|████████▏ | 115/140 [00:39<00:08,  2.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  83%|████████▎ | 116/140 [00:40<00:08,  2.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  84%|████████▎ | 117/140 [00:40<00:08,  2.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  84%|████████▍ | 118/140 [00:40<00:07,  2.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  85%|████████▌ | 119/140 [00:41<00:07,  2.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  86%|████████▌ | 120/140 [00:41<00:07,  2.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  86%|████████▋ | 121/140 [00:41<00:06,  2.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  87%|████████▋ | 122/140 [00:42<00:06,  2.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  88%|████████▊ | 123/140 [00:42<00:06,  2.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  89%|████████▊ | 124/140 [00:43<00:05,  2.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  89%|████████▉ | 125/140 [00:43<00:05,  2.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  90%|█████████ | 126/140 [00:43<00:04,  2.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  91%|█████████ | 127/140 [00:44<00:04,  2.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  91%|█████████▏| 128/140 [00:44<00:04,  2.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  92%|█████████▏| 129/140 [00:44<00:03,  2.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  93%|█████████▎| 130/140 [00:45<00:03,  2.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  94%|█████████▎| 131/140 [00:45<00:03,  2.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  94%|█████████▍| 132/140 [00:45<00:02,  2.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  95%|█████████▌| 133/140 [00:46<00:02,  2.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  96%|█████████▌| 134/140 [00:46<00:02,  2.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  96%|█████████▋| 135/140 [00:46<00:01,  2.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  97%|█████████▋| 136/140 [00:47<00:01,  2.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  98%|█████████▊| 137/140 [00:47<00:01,  2.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  99%|█████████▊| 138/140 [00:47<00:00,  2.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  99%|█████████▉| 139/140 [00:48<00:00,  2.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([30, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([30, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([30, 768])\n",
            "Final output shape: torch.Size([30, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 140/140 [00:48<00:00,  2.88it/s]\n",
            "Valid:   3%|▎         | 1/30 [00:00<00:07,  3.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  10%|█         | 3/30 [00:00<00:04,  6.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  17%|█▋        | 5/30 [00:00<00:03,  7.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  23%|██▎       | 7/30 [00:01<00:02,  7.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  30%|███       | 9/30 [00:01<00:02,  7.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  37%|███▋      | 11/30 [00:01<00:02,  7.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  43%|████▎     | 13/30 [00:01<00:02,  8.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  50%|█████     | 15/30 [00:01<00:01,  8.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  57%|█████▋    | 17/30 [00:02<00:01,  8.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  63%|██████▎   | 19/30 [00:02<00:01,  8.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  70%|███████   | 21/30 [00:02<00:01,  8.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  77%|███████▋  | 23/30 [00:02<00:00,  8.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  83%|████████▎ | 25/30 [00:03<00:00,  8.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  90%|█████████ | 27/30 [00:03<00:00,  8.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 30/30 [00:03<00:00,  7.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([10, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([10, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([10, 768])\n",
            "Final output shape: torch.Size([10, 768])\n",
            "Epoch 2: train_acc=0.647, val_acc=0.718\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   0%|          | 0/140 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   1%|          | 1/140 [00:00<01:06,  2.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   1%|▏         | 2/140 [00:00<00:55,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   2%|▏         | 3/140 [00:01<00:53,  2.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   3%|▎         | 4/140 [00:01<00:51,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   4%|▎         | 5/140 [00:01<00:49,  2.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   4%|▍         | 6/140 [00:02<00:49,  2.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   5%|▌         | 7/140 [00:02<00:48,  2.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   6%|▌         | 8/140 [00:02<00:47,  2.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   6%|▋         | 9/140 [00:03<00:47,  2.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   7%|▋         | 10/140 [00:03<00:46,  2.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   8%|▊         | 11/140 [00:04<00:46,  2.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   9%|▊         | 12/140 [00:04<00:46,  2.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   9%|▉         | 13/140 [00:04<00:45,  2.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  10%|█         | 14/140 [00:05<00:45,  2.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  11%|█         | 15/140 [00:05<00:45,  2.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  11%|█▏        | 16/140 [00:05<00:44,  2.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  12%|█▏        | 17/140 [00:06<00:44,  2.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  13%|█▎        | 18/140 [00:06<00:44,  2.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  14%|█▎        | 19/140 [00:06<00:43,  2.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  14%|█▍        | 20/140 [00:07<00:43,  2.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  15%|█▌        | 21/140 [00:07<00:43,  2.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  16%|█▌        | 22/140 [00:08<00:42,  2.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  16%|█▋        | 23/140 [00:08<00:42,  2.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  17%|█▋        | 24/140 [00:08<00:42,  2.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  18%|█▊        | 25/140 [00:09<00:41,  2.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  19%|█▊        | 26/140 [00:09<00:41,  2.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  19%|█▉        | 27/140 [00:09<00:41,  2.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  20%|██        | 28/140 [00:10<00:40,  2.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  21%|██        | 29/140 [00:10<00:40,  2.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  21%|██▏       | 30/140 [00:10<00:39,  2.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  22%|██▏       | 31/140 [00:11<00:39,  2.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  23%|██▎       | 32/140 [00:11<00:39,  2.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  24%|██▎       | 33/140 [00:12<00:38,  2.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  24%|██▍       | 34/140 [00:12<00:38,  2.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  25%|██▌       | 35/140 [00:12<00:38,  2.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  26%|██▌       | 36/140 [00:13<00:37,  2.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  26%|██▋       | 37/140 [00:13<00:37,  2.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  27%|██▋       | 38/140 [00:13<00:36,  2.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  28%|██▊       | 39/140 [00:14<00:36,  2.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  29%|██▊       | 40/140 [00:14<00:36,  2.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  29%|██▉       | 41/140 [00:14<00:35,  2.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  30%|███       | 42/140 [00:15<00:35,  2.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  31%|███       | 43/140 [00:15<00:35,  2.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  31%|███▏      | 44/140 [00:16<00:34,  2.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  32%|███▏      | 45/140 [00:16<00:34,  2.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  33%|███▎      | 46/140 [00:16<00:34,  2.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  34%|███▎      | 47/140 [00:17<00:34,  2.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  34%|███▍      | 48/140 [00:17<00:33,  2.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  35%|███▌      | 49/140 [00:17<00:33,  2.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  36%|███▌      | 50/140 [00:18<00:33,  2.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  36%|███▋      | 51/140 [00:18<00:32,  2.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  37%|███▋      | 52/140 [00:19<00:32,  2.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  38%|███▊      | 53/140 [00:19<00:32,  2.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  39%|███▊      | 54/140 [00:19<00:31,  2.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  39%|███▉      | 55/140 [00:20<00:31,  2.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  40%|████      | 56/140 [00:20<00:31,  2.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  41%|████      | 57/140 [00:20<00:30,  2.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  41%|████▏     | 58/140 [00:21<00:30,  2.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  42%|████▏     | 59/140 [00:21<00:30,  2.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  43%|████▎     | 60/140 [00:21<00:29,  2.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  44%|████▎     | 61/140 [00:22<00:29,  2.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  44%|████▍     | 62/140 [00:22<00:29,  2.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  45%|████▌     | 63/140 [00:23<00:28,  2.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  46%|████▌     | 64/140 [00:23<00:28,  2.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  46%|████▋     | 65/140 [00:23<00:28,  2.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  47%|████▋     | 66/140 [00:24<00:27,  2.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  48%|████▊     | 67/140 [00:24<00:27,  2.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  49%|████▊     | 68/140 [00:24<00:27,  2.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  49%|████▉     | 69/140 [00:25<00:26,  2.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  50%|█████     | 70/140 [00:25<00:26,  2.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  51%|█████     | 71/140 [00:26<00:26,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  51%|█████▏    | 72/140 [00:26<00:25,  2.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  52%|█████▏    | 73/140 [00:26<00:25,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  53%|█████▎    | 74/140 [00:27<00:24,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  54%|█████▎    | 75/140 [00:27<00:24,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  54%|█████▍    | 76/140 [00:27<00:24,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  55%|█████▌    | 77/140 [00:28<00:23,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  56%|█████▌    | 78/140 [00:28<00:23,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  56%|█████▋    | 79/140 [00:29<00:23,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  57%|█████▋    | 80/140 [00:29<00:22,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  58%|█████▊    | 81/140 [00:29<00:22,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  59%|█████▊    | 82/140 [00:30<00:22,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  59%|█████▉    | 83/140 [00:30<00:21,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  60%|██████    | 84/140 [00:31<00:21,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  61%|██████    | 85/140 [00:31<00:21,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  61%|██████▏   | 86/140 [00:31<00:20,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  62%|██████▏   | 87/140 [00:32<00:20,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  63%|██████▎   | 88/140 [00:32<00:20,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  64%|██████▎   | 89/140 [00:32<00:19,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  64%|██████▍   | 90/140 [00:33<00:19,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  65%|██████▌   | 91/140 [00:33<00:18,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  66%|██████▌   | 92/140 [00:34<00:18,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  66%|██████▋   | 93/140 [00:34<00:18,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  67%|██████▋   | 94/140 [00:34<00:17,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  68%|██████▊   | 95/140 [00:35<00:17,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  69%|██████▊   | 96/140 [00:35<00:17,  2.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  69%|██████▉   | 97/140 [00:36<00:16,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  70%|███████   | 98/140 [00:36<00:16,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  71%|███████   | 99/140 [00:36<00:15,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  71%|███████▏  | 100/140 [00:37<00:15,  2.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  72%|███████▏  | 101/140 [00:37<00:15,  2.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  73%|███████▎  | 102/140 [00:38<00:14,  2.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  74%|███████▎  | 103/140 [00:38<00:14,  2.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  74%|███████▍  | 104/140 [00:38<00:14,  2.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  75%|███████▌  | 105/140 [00:39<00:13,  2.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  76%|███████▌  | 106/140 [00:39<00:13,  2.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  76%|███████▋  | 107/140 [00:40<00:13,  2.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  77%|███████▋  | 108/140 [00:40<00:12,  2.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  78%|███████▊  | 109/140 [00:40<00:12,  2.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  79%|███████▊  | 110/140 [00:41<00:11,  2.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  79%|███████▉  | 111/140 [00:41<00:11,  2.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  80%|████████  | 112/140 [00:42<00:11,  2.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  81%|████████  | 113/140 [00:42<00:10,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  81%|████████▏ | 114/140 [00:42<00:10,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  82%|████████▏ | 115/140 [00:43<00:09,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  83%|████████▎ | 116/140 [00:43<00:09,  2.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  84%|████████▎ | 117/140 [00:44<00:09,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  84%|████████▍ | 118/140 [00:44<00:08,  2.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  85%|████████▌ | 119/140 [00:44<00:08,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  86%|████████▌ | 120/140 [00:45<00:07,  2.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  86%|████████▋ | 121/140 [00:45<00:07,  2.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  87%|████████▋ | 122/140 [00:46<00:07,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  88%|████████▊ | 123/140 [00:46<00:06,  2.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  89%|████████▊ | 124/140 [00:46<00:06,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  89%|████████▉ | 125/140 [00:47<00:06,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  90%|█████████ | 126/140 [00:47<00:05,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  91%|█████████ | 127/140 [00:48<00:05,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  91%|█████████▏| 128/140 [00:48<00:04,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  92%|█████████▏| 129/140 [00:48<00:04,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  93%|█████████▎| 130/140 [00:49<00:04,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  94%|█████████▎| 131/140 [00:49<00:03,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  94%|█████████▍| 132/140 [00:50<00:03,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  95%|█████████▌| 133/140 [00:50<00:02,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  96%|█████████▌| 134/140 [00:50<00:02,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  96%|█████████▋| 135/140 [00:51<00:02,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  97%|█████████▋| 136/140 [00:51<00:01,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  98%|█████████▊| 137/140 [00:52<00:01,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  99%|█████████▊| 138/140 [00:52<00:00,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  99%|█████████▉| 139/140 [00:52<00:00,  2.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([30, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([30, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([30, 768])\n",
            "Final output shape: torch.Size([30, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 140/140 [00:53<00:00,  2.63it/s]\n",
            "Valid:   3%|▎         | 1/30 [00:00<00:07,  3.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  10%|█         | 3/30 [00:00<00:04,  6.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  17%|█▋        | 5/30 [00:00<00:03,  6.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  23%|██▎       | 7/30 [00:01<00:03,  6.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  30%|███       | 9/30 [00:01<00:03,  6.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  37%|███▋      | 11/30 [00:01<00:02,  7.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  43%|████▎     | 13/30 [00:01<00:02,  7.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  50%|█████     | 15/30 [00:02<00:02,  7.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  57%|█████▋    | 17/30 [00:02<00:01,  7.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  63%|██████▎   | 19/30 [00:02<00:01,  7.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  70%|███████   | 21/30 [00:03<00:01,  7.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  77%|███████▋  | 23/30 [00:03<00:00,  7.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  83%|████████▎ | 25/30 [00:03<00:00,  7.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  90%|█████████ | 27/30 [00:03<00:00,  7.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 30/30 [00:04<00:00,  7.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([10, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([10, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([10, 768])\n",
            "Final output shape: torch.Size([10, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: train_acc=0.761, val_acc=0.748\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   0%|          | 0/140 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   1%|          | 1/140 [00:00<01:09,  1.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   1%|▏         | 2/140 [00:00<00:59,  2.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   2%|▏         | 3/140 [00:01<00:58,  2.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   3%|▎         | 4/140 [00:01<00:55,  2.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   4%|▎         | 5/140 [00:02<00:54,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   4%|▍         | 6/140 [00:02<00:53,  2.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   5%|▌         | 7/140 [00:02<00:52,  2.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   6%|▌         | 8/140 [00:03<00:52,  2.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   6%|▋         | 9/140 [00:03<00:51,  2.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   7%|▋         | 10/140 [00:04<00:50,  2.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   8%|▊         | 11/140 [00:04<00:50,  2.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   9%|▊         | 12/140 [00:04<00:49,  2.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   9%|▉         | 13/140 [00:05<00:49,  2.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  10%|█         | 14/140 [00:05<00:48,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  11%|█         | 15/140 [00:05<00:48,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  11%|█▏        | 16/140 [00:06<00:48,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  12%|█▏        | 17/140 [00:06<00:47,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  13%|█▎        | 18/140 [00:07<00:47,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  14%|█▎        | 19/140 [00:07<00:46,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  14%|█▍        | 20/140 [00:07<00:46,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  15%|█▌        | 21/140 [00:08<00:45,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  16%|█▌        | 22/140 [00:08<00:45,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  16%|█▋        | 23/140 [00:09<00:45,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  17%|█▋        | 24/140 [00:09<00:44,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  18%|█▊        | 25/140 [00:09<00:44,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  19%|█▊        | 26/140 [00:10<00:43,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  19%|█▉        | 27/140 [00:10<00:43,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  20%|██        | 28/140 [00:10<00:42,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  21%|██        | 29/140 [00:11<00:42,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  21%|██▏       | 30/140 [00:11<00:42,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  22%|██▏       | 31/140 [00:12<00:41,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  23%|██▎       | 32/140 [00:12<00:41,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  24%|██▎       | 33/140 [00:12<00:40,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  24%|██▍       | 34/140 [00:13<00:40,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  25%|██▌       | 35/140 [00:13<00:40,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  26%|██▌       | 36/140 [00:14<00:39,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  26%|██▋       | 37/140 [00:14<00:39,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  27%|██▋       | 38/140 [00:14<00:38,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  28%|██▊       | 39/140 [00:15<00:38,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  29%|██▊       | 40/140 [00:15<00:37,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  29%|██▉       | 41/140 [00:15<00:37,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  30%|███       | 42/140 [00:16<00:37,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  31%|███       | 43/140 [00:16<00:36,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  31%|███▏      | 44/140 [00:17<00:36,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  32%|███▏      | 45/140 [00:17<00:35,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  33%|███▎      | 46/140 [00:17<00:35,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  34%|███▎      | 47/140 [00:18<00:35,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  34%|███▍      | 48/140 [00:18<00:34,  2.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  35%|███▌      | 49/140 [00:18<00:34,  2.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  36%|███▌      | 50/140 [00:19<00:33,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  36%|███▋      | 51/140 [00:19<00:33,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  37%|███▋      | 52/140 [00:20<00:33,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  38%|███▊      | 53/140 [00:20<00:32,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  39%|███▊      | 54/140 [00:20<00:32,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  39%|███▉      | 55/140 [00:21<00:32,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  40%|████      | 56/140 [00:21<00:31,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  41%|████      | 57/140 [00:21<00:31,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  41%|████▏     | 58/140 [00:22<00:30,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  42%|████▏     | 59/140 [00:22<00:30,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  43%|████▎     | 60/140 [00:23<00:30,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  44%|████▎     | 61/140 [00:23<00:29,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  44%|████▍     | 62/140 [00:23<00:29,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  45%|████▌     | 63/140 [00:24<00:29,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  46%|████▌     | 64/140 [00:24<00:28,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  46%|████▋     | 65/140 [00:24<00:28,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  47%|████▋     | 66/140 [00:25<00:27,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  48%|████▊     | 67/140 [00:25<00:27,  2.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  49%|████▊     | 68/140 [00:26<00:27,  2.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  49%|████▉     | 69/140 [00:26<00:26,  2.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  50%|█████     | 70/140 [00:26<00:26,  2.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  51%|█████     | 71/140 [00:27<00:25,  2.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  51%|█████▏    | 72/140 [00:27<00:25,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  52%|█████▏    | 73/140 [00:27<00:25,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  53%|█████▎    | 74/140 [00:28<00:24,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  54%|█████▎    | 75/140 [00:28<00:24,  2.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  54%|█████▍    | 76/140 [00:29<00:24,  2.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  55%|█████▌    | 77/140 [00:29<00:23,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  56%|█████▌    | 78/140 [00:29<00:23,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  56%|█████▋    | 79/140 [00:30<00:22,  2.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  57%|█████▋    | 80/140 [00:30<00:22,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  58%|█████▊    | 81/140 [00:31<00:22,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  59%|█████▊    | 82/140 [00:31<00:21,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  59%|█████▉    | 83/140 [00:31<00:21,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  60%|██████    | 84/140 [00:32<00:21,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  61%|██████    | 85/140 [00:32<00:20,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  61%|██████▏   | 86/140 [00:32<00:20,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  62%|██████▏   | 87/140 [00:33<00:20,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  63%|██████▎   | 88/140 [00:33<00:19,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  64%|██████▎   | 89/140 [00:34<00:19,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  64%|██████▍   | 90/140 [00:34<00:18,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  65%|██████▌   | 91/140 [00:34<00:18,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  66%|██████▌   | 92/140 [00:35<00:18,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  66%|██████▋   | 93/140 [00:35<00:17,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  67%|██████▋   | 94/140 [00:35<00:17,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  68%|██████▊   | 95/140 [00:36<00:17,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  69%|██████▊   | 96/140 [00:36<00:16,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  69%|██████▉   | 97/140 [00:37<00:16,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  70%|███████   | 98/140 [00:37<00:15,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  71%|███████   | 99/140 [00:37<00:15,  2.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  71%|███████▏  | 100/140 [00:38<00:15,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  72%|███████▏  | 101/140 [00:38<00:14,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  73%|███████▎  | 102/140 [00:38<00:14,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  74%|███████▎  | 103/140 [00:39<00:14,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  74%|███████▍  | 104/140 [00:39<00:13,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  75%|███████▌  | 105/140 [00:40<00:13,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  76%|███████▌  | 106/140 [00:40<00:12,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  76%|███████▋  | 107/140 [00:40<00:12,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  77%|███████▋  | 108/140 [00:41<00:12,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  78%|███████▊  | 109/140 [00:41<00:11,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  79%|███████▊  | 110/140 [00:41<00:11,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  79%|███████▉  | 111/140 [00:42<00:10,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  80%|████████  | 112/140 [00:42<00:10,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  81%|████████  | 113/140 [00:43<00:10,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  81%|████████▏ | 114/140 [00:43<00:09,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  82%|████████▏ | 115/140 [00:43<00:09,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  83%|████████▎ | 116/140 [00:44<00:09,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  84%|████████▎ | 117/140 [00:44<00:08,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  84%|████████▍ | 118/140 [00:45<00:08,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  85%|████████▌ | 119/140 [00:45<00:08,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  86%|████████▌ | 120/140 [00:45<00:07,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  86%|████████▋ | 121/140 [00:46<00:07,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  87%|████████▋ | 122/140 [00:46<00:06,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  88%|████████▊ | 123/140 [00:46<00:06,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  89%|████████▊ | 124/140 [00:47<00:06,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  89%|████████▉ | 125/140 [00:47<00:05,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  90%|█████████ | 126/140 [00:48<00:05,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  91%|█████████ | 127/140 [00:48<00:04,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  91%|█████████▏| 128/140 [00:48<00:04,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  92%|█████████▏| 129/140 [00:49<00:04,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  93%|█████████▎| 130/140 [00:49<00:03,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  94%|█████████▎| 131/140 [00:49<00:03,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  94%|█████████▍| 132/140 [00:50<00:03,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  95%|█████████▌| 133/140 [00:50<00:02,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  96%|█████████▌| 134/140 [00:51<00:02,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  96%|█████████▋| 135/140 [00:51<00:01,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  97%|█████████▋| 136/140 [00:51<00:01,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  98%|█████████▊| 137/140 [00:52<00:01,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  99%|█████████▊| 138/140 [00:52<00:00,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  99%|█████████▉| 139/140 [00:53<00:00,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([30, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([30, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([30, 768])\n",
            "Final output shape: torch.Size([30, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 140/140 [00:53<00:00,  2.62it/s]\n",
            "Valid:   3%|▎         | 1/30 [00:00<00:07,  3.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  10%|█         | 3/30 [00:00<00:04,  6.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  17%|█▋        | 5/30 [00:00<00:03,  7.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  23%|██▎       | 7/30 [00:01<00:03,  6.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  30%|███       | 9/30 [00:01<00:02,  7.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  37%|███▋      | 11/30 [00:01<00:02,  7.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  43%|████▎     | 13/30 [00:01<00:02,  7.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  50%|█████     | 15/30 [00:02<00:02,  7.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  57%|█████▋    | 17/30 [00:02<00:01,  7.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  63%|██████▎   | 19/30 [00:02<00:01,  7.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  70%|███████   | 21/30 [00:02<00:01,  7.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  77%|███████▋  | 23/30 [00:03<00:00,  7.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  83%|████████▎ | 25/30 [00:03<00:00,  7.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  90%|█████████ | 27/30 [00:03<00:00,  7.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 30/30 [00:04<00:00,  7.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([10, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([10, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([10, 768])\n",
            "Final output shape: torch.Size([10, 768])\n",
            "Epoch 4: train_acc=0.826, val_acc=0.756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   0%|          | 0/140 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   1%|          | 1/140 [00:00<01:09,  1.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   1%|▏         | 2/140 [00:00<00:58,  2.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   2%|▏         | 3/140 [00:01<00:57,  2.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   3%|▎         | 4/140 [00:01<00:54,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   4%|▎         | 5/140 [00:02<00:53,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   4%|▍         | 6/140 [00:02<00:52,  2.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   5%|▌         | 7/140 [00:02<00:51,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   6%|▌         | 8/140 [00:03<00:51,  2.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   6%|▋         | 9/140 [00:03<00:51,  2.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   7%|▋         | 10/140 [00:03<00:50,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   8%|▊         | 11/140 [00:04<00:50,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   9%|▊         | 12/140 [00:04<00:49,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   9%|▉         | 13/140 [00:05<00:49,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  10%|█         | 14/140 [00:05<00:48,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  11%|█         | 15/140 [00:05<00:48,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  11%|█▏        | 16/140 [00:06<00:47,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  12%|█▏        | 17/140 [00:06<00:47,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  13%|█▎        | 18/140 [00:07<00:47,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  14%|█▎        | 19/140 [00:07<00:47,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  14%|█▍        | 20/140 [00:07<00:46,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  15%|█▌        | 21/140 [00:08<00:46,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  16%|█▌        | 22/140 [00:08<00:45,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  16%|█▋        | 23/140 [00:09<00:45,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  17%|█▋        | 24/140 [00:09<00:45,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  18%|█▊        | 25/140 [00:09<00:44,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  19%|█▊        | 26/140 [00:10<00:44,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  19%|█▉        | 27/140 [00:10<00:43,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  20%|██        | 28/140 [00:10<00:43,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  21%|██        | 29/140 [00:11<00:43,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  21%|██▏       | 30/140 [00:11<00:42,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  22%|██▏       | 31/140 [00:12<00:42,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  23%|██▎       | 32/140 [00:12<00:41,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  24%|██▎       | 33/140 [00:12<00:41,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  24%|██▍       | 34/140 [00:13<00:41,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  25%|██▌       | 35/140 [00:13<00:40,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  26%|██▌       | 36/140 [00:14<00:40,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  26%|██▋       | 37/140 [00:14<00:40,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  27%|██▋       | 38/140 [00:14<00:39,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  28%|██▊       | 39/140 [00:15<00:39,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  29%|██▊       | 40/140 [00:15<00:38,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  29%|██▉       | 41/140 [00:16<00:38,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  30%|███       | 42/140 [00:16<00:38,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  31%|███       | 43/140 [00:16<00:37,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  31%|███▏      | 44/140 [00:17<00:37,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  32%|███▏      | 45/140 [00:17<00:36,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  33%|███▎      | 46/140 [00:17<00:36,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  34%|███▎      | 47/140 [00:18<00:36,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  34%|███▍      | 48/140 [00:18<00:35,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  35%|███▌      | 49/140 [00:19<00:35,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  36%|███▌      | 50/140 [00:19<00:34,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  36%|███▋      | 51/140 [00:19<00:34,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  37%|███▋      | 52/140 [00:20<00:34,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  38%|███▊      | 53/140 [00:20<00:33,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  39%|███▊      | 54/140 [00:21<00:33,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  39%|███▉      | 55/140 [00:21<00:32,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  40%|████      | 56/140 [00:21<00:32,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  41%|████      | 57/140 [00:22<00:32,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  41%|████▏     | 58/140 [00:22<00:31,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  42%|████▏     | 59/140 [00:22<00:31,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  43%|████▎     | 60/140 [00:23<00:30,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  44%|████▎     | 61/140 [00:23<00:30,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  44%|████▍     | 62/140 [00:24<00:29,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  45%|████▌     | 63/140 [00:24<00:29,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  46%|████▌     | 64/140 [00:24<00:29,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  46%|████▋     | 65/140 [00:25<00:28,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  47%|████▋     | 66/140 [00:25<00:28,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  48%|████▊     | 67/140 [00:26<00:28,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  49%|████▊     | 68/140 [00:26<00:27,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  49%|████▉     | 69/140 [00:26<00:27,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  50%|█████     | 70/140 [00:27<00:26,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  51%|█████     | 71/140 [00:27<00:26,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  51%|█████▏    | 72/140 [00:27<00:26,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  52%|█████▏    | 73/140 [00:28<00:25,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  53%|█████▎    | 74/140 [00:28<00:25,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  54%|█████▎    | 75/140 [00:29<00:25,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  54%|█████▍    | 76/140 [00:29<00:24,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  55%|█████▌    | 77/140 [00:29<00:24,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  56%|█████▌    | 78/140 [00:30<00:23,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  56%|█████▋    | 79/140 [00:30<00:23,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  57%|█████▋    | 80/140 [00:31<00:23,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  58%|█████▊    | 81/140 [00:31<00:22,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  59%|█████▊    | 82/140 [00:31<00:22,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  59%|█████▉    | 83/140 [00:32<00:21,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  60%|██████    | 84/140 [00:32<00:21,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  61%|██████    | 85/140 [00:32<00:21,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  61%|██████▏   | 86/140 [00:33<00:20,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  62%|██████▏   | 87/140 [00:33<00:20,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  63%|██████▎   | 88/140 [00:34<00:19,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  64%|██████▎   | 89/140 [00:34<00:19,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  64%|██████▍   | 90/140 [00:34<00:19,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  65%|██████▌   | 91/140 [00:35<00:18,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  66%|██████▌   | 92/140 [00:35<00:18,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  66%|██████▋   | 93/140 [00:36<00:18,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  67%|██████▋   | 94/140 [00:36<00:17,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  68%|██████▊   | 95/140 [00:36<00:17,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  69%|██████▊   | 96/140 [00:37<00:16,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  69%|██████▉   | 97/140 [00:37<00:16,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  70%|███████   | 98/140 [00:37<00:16,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  71%|███████   | 99/140 [00:38<00:15,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  71%|███████▏  | 100/140 [00:38<00:15,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  72%|███████▏  | 101/140 [00:39<00:14,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  73%|███████▎  | 102/140 [00:39<00:14,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  74%|███████▎  | 103/140 [00:39<00:14,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  74%|███████▍  | 104/140 [00:40<00:13,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  75%|███████▌  | 105/140 [00:40<00:13,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  76%|███████▌  | 106/140 [00:41<00:13,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  76%|███████▋  | 107/140 [00:41<00:12,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  77%|███████▋  | 108/140 [00:41<00:12,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  78%|███████▊  | 109/140 [00:42<00:11,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  79%|███████▊  | 110/140 [00:42<00:11,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  79%|███████▉  | 111/140 [00:42<00:11,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  80%|████████  | 112/140 [00:43<00:10,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  81%|████████  | 113/140 [00:43<00:10,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  81%|████████▏ | 114/140 [00:44<00:09,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  82%|████████▏ | 115/140 [00:44<00:09,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  83%|████████▎ | 116/140 [00:44<00:09,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  84%|████████▎ | 117/140 [00:45<00:08,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  84%|████████▍ | 118/140 [00:45<00:08,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  85%|████████▌ | 119/140 [00:46<00:08,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  86%|████████▌ | 120/140 [00:46<00:07,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  86%|████████▋ | 121/140 [00:46<00:07,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  87%|████████▋ | 122/140 [00:47<00:06,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  88%|████████▊ | 123/140 [00:47<00:06,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  89%|████████▊ | 124/140 [00:47<00:06,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  89%|████████▉ | 125/140 [00:48<00:05,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  90%|█████████ | 126/140 [00:48<00:05,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  91%|█████████ | 127/140 [00:49<00:04,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  91%|█████████▏| 128/140 [00:49<00:04,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  92%|█████████▏| 129/140 [00:49<00:04,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  93%|█████████▎| 130/140 [00:50<00:03,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  94%|█████████▎| 131/140 [00:50<00:03,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  94%|█████████▍| 132/140 [00:51<00:03,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  95%|█████████▌| 133/140 [00:51<00:02,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  96%|█████████▌| 134/140 [00:51<00:02,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  96%|█████████▋| 135/140 [00:52<00:01,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  97%|█████████▋| 136/140 [00:52<00:01,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  98%|█████████▊| 137/140 [00:52<00:01,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  99%|█████████▊| 138/140 [00:53<00:00,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  99%|█████████▉| 139/140 [00:53<00:00,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([30, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([30, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([30, 768])\n",
            "Final output shape: torch.Size([30, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 140/140 [00:54<00:00,  2.59it/s]\n",
            "Valid:   3%|▎         | 1/30 [00:00<00:07,  3.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  10%|█         | 3/30 [00:00<00:04,  6.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  17%|█▋        | 5/30 [00:00<00:03,  7.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  23%|██▎       | 7/30 [00:01<00:03,  6.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  30%|███       | 9/30 [00:01<00:02,  7.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  37%|███▋      | 11/30 [00:01<00:02,  7.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  43%|████▎     | 13/30 [00:01<00:02,  7.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  50%|█████     | 15/30 [00:02<00:02,  7.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  57%|█████▋    | 17/30 [00:02<00:01,  7.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  63%|██████▎   | 19/30 [00:02<00:01,  7.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  70%|███████   | 21/30 [00:02<00:01,  7.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  77%|███████▋  | 23/30 [00:03<00:00,  7.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  83%|████████▎ | 25/30 [00:03<00:00,  7.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  90%|█████████ | 27/30 [00:03<00:00,  7.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  97%|█████████▋| 29/30 [00:03<00:00,  7.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([10, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([10, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([10, 768])\n",
            "Final output shape: torch.Size([10, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rValid: 100%|██████████| 30/30 [00:04<00:00,  7.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: train_acc=0.870, val_acc=0.714\n",
            "Model saved to trained_models/dino2d_mean_epochs_5_model.pth\n",
            "\n",
            "--- Processing backbone: dino2d, temporal pooling: attention ---\n",
            "Shape of data before calling get_train_val_test_loaders: (62, 80, 1, 100, 100)\n",
            "Length of train_idx: 43\n",
            "get_train_val_test_loaders: Shape of data and labels: (62, 80, 1, 100, 100), (62,)\n",
            "get_train_val_test_loaders: Length of train_idx: 43\n",
            "Training new model for backbone: dino2d, temporal pooling: attention\n",
            "####### auto\n",
            "--- build_backbone -- dino2d\n",
            "--- build_backbone model name:  openai/clip-vit-base-patch32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   0%|          | 0/140 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   1%|          | 1/140 [00:00<01:19,  1.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   1%|▏         | 2/140 [00:00<01:02,  2.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   2%|▏         | 3/140 [00:01<00:57,  2.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   3%|▎         | 4/140 [00:01<00:54,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   4%|▎         | 5/140 [00:02<00:52,  2.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   4%|▍         | 6/140 [00:02<00:52,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   5%|▌         | 7/140 [00:02<00:51,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   6%|▌         | 8/140 [00:03<00:50,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   6%|▋         | 9/140 [00:03<00:50,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   7%|▋         | 10/140 [00:03<00:49,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   8%|▊         | 11/140 [00:04<00:49,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   9%|▊         | 12/140 [00:04<00:48,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   9%|▉         | 13/140 [00:05<00:48,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  10%|█         | 14/140 [00:05<00:47,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  11%|█         | 15/140 [00:05<00:47,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  11%|█▏        | 16/140 [00:06<00:46,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  12%|█▏        | 17/140 [00:06<00:46,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  13%|█▎        | 18/140 [00:07<00:46,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  14%|█▎        | 19/140 [00:07<00:45,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  14%|█▍        | 20/140 [00:07<00:45,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  15%|█▌        | 21/140 [00:08<00:45,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  16%|█▌        | 22/140 [00:08<00:44,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  16%|█▋        | 23/140 [00:08<00:44,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  17%|█▋        | 24/140 [00:09<00:44,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  18%|█▊        | 25/140 [00:09<00:43,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  19%|█▊        | 26/140 [00:10<00:43,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  19%|█▉        | 27/140 [00:10<00:43,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  20%|██        | 28/140 [00:10<00:42,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  21%|██        | 29/140 [00:11<00:42,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  21%|██▏       | 30/140 [00:11<00:41,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  22%|██▏       | 31/140 [00:11<00:41,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  23%|██▎       | 32/140 [00:12<00:41,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  24%|██▎       | 33/140 [00:12<00:40,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  24%|██▍       | 34/140 [00:13<00:40,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  25%|██▌       | 35/140 [00:13<00:39,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  26%|██▌       | 36/140 [00:13<00:39,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  26%|██▋       | 37/140 [00:14<00:39,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  27%|██▋       | 38/140 [00:14<00:38,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  28%|██▊       | 39/140 [00:15<00:38,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  29%|██▊       | 40/140 [00:15<00:38,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  29%|██▉       | 41/140 [00:15<00:37,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  30%|███       | 42/140 [00:16<00:37,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  31%|███       | 43/140 [00:16<00:37,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  31%|███▏      | 44/140 [00:16<00:36,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  32%|███▏      | 45/140 [00:17<00:36,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  33%|███▎      | 46/140 [00:17<00:35,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  34%|███▎      | 47/140 [00:18<00:35,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  34%|███▍      | 48/140 [00:18<00:35,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  35%|███▌      | 49/140 [00:18<00:34,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  36%|███▌      | 50/140 [00:19<00:34,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  36%|███▋      | 51/140 [00:19<00:34,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  37%|███▋      | 52/140 [00:19<00:33,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  38%|███▊      | 53/140 [00:20<00:33,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  39%|███▊      | 54/140 [00:20<00:32,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  39%|███▉      | 55/140 [00:21<00:32,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  40%|████      | 56/140 [00:21<00:32,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  41%|████      | 57/140 [00:21<00:31,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  41%|████▏     | 58/140 [00:22<00:31,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  42%|████▏     | 59/140 [00:22<00:31,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  43%|████▎     | 60/140 [00:23<00:30,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  44%|████▎     | 61/140 [00:23<00:30,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  44%|████▍     | 62/140 [00:23<00:29,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  45%|████▌     | 63/140 [00:24<00:29,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  46%|████▌     | 64/140 [00:24<00:29,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  46%|████▋     | 65/140 [00:24<00:28,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  47%|████▋     | 66/140 [00:25<00:28,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  48%|████▊     | 67/140 [00:25<00:28,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  49%|████▊     | 68/140 [00:26<00:27,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  49%|████▉     | 69/140 [00:26<00:27,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  50%|█████     | 70/140 [00:26<00:26,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  51%|█████     | 71/140 [00:27<00:26,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  51%|█████▏    | 72/140 [00:27<00:26,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  52%|█████▏    | 73/140 [00:28<00:25,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  53%|█████▎    | 74/140 [00:28<00:25,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  54%|█████▎    | 75/140 [00:28<00:24,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  54%|█████▍    | 76/140 [00:29<00:24,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  55%|█████▌    | 77/140 [00:29<00:24,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  56%|█████▌    | 78/140 [00:29<00:23,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  56%|█████▋    | 79/140 [00:30<00:23,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  57%|█████▋    | 80/140 [00:30<00:23,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  58%|█████▊    | 81/140 [00:31<00:22,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  59%|█████▊    | 82/140 [00:31<00:22,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  59%|█████▉    | 83/140 [00:31<00:21,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  60%|██████    | 84/140 [00:32<00:21,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  61%|██████    | 85/140 [00:32<00:21,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  61%|██████▏   | 86/140 [00:33<00:20,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  62%|██████▏   | 87/140 [00:33<00:20,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  63%|██████▎   | 88/140 [00:33<00:19,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  64%|██████▎   | 89/140 [00:34<00:19,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  64%|██████▍   | 90/140 [00:34<00:19,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  65%|██████▌   | 91/140 [00:34<00:18,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  66%|██████▌   | 92/140 [00:35<00:18,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  66%|██████▋   | 93/140 [00:35<00:18,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  67%|██████▋   | 94/140 [00:36<00:17,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  68%|██████▊   | 95/140 [00:36<00:17,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  69%|██████▊   | 96/140 [00:36<00:16,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  69%|██████▉   | 97/140 [00:37<00:16,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  70%|███████   | 98/140 [00:37<00:16,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  71%|███████   | 99/140 [00:38<00:15,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  71%|███████▏  | 100/140 [00:38<00:15,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  72%|███████▏  | 101/140 [00:38<00:14,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  73%|███████▎  | 102/140 [00:39<00:14,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  74%|███████▎  | 103/140 [00:39<00:14,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  74%|███████▍  | 104/140 [00:39<00:13,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  75%|███████▌  | 105/140 [00:40<00:13,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  76%|███████▌  | 106/140 [00:40<00:13,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  76%|███████▋  | 107/140 [00:41<00:12,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  77%|███████▋  | 108/140 [00:41<00:12,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  78%|███████▊  | 109/140 [00:41<00:11,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  79%|███████▊  | 110/140 [00:42<00:11,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  79%|███████▉  | 111/140 [00:42<00:11,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  80%|████████  | 112/140 [00:42<00:10,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  81%|████████  | 113/140 [00:43<00:10,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  81%|████████▏ | 114/140 [00:43<00:09,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  82%|████████▏ | 115/140 [00:44<00:09,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  83%|████████▎ | 116/140 [00:44<00:09,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  84%|████████▎ | 117/140 [00:44<00:08,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  84%|████████▍ | 118/140 [00:45<00:08,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  85%|████████▌ | 119/140 [00:45<00:08,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  86%|████████▌ | 120/140 [00:46<00:07,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  86%|████████▋ | 121/140 [00:46<00:07,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  87%|████████▋ | 122/140 [00:46<00:06,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  88%|████████▊ | 123/140 [00:47<00:06,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  89%|████████▊ | 124/140 [00:47<00:06,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  89%|████████▉ | 125/140 [00:47<00:05,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  90%|█████████ | 126/140 [00:48<00:05,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  91%|█████████ | 127/140 [00:48<00:04,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  91%|█████████▏| 128/140 [00:49<00:04,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  92%|█████████▏| 129/140 [00:49<00:04,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  93%|█████████▎| 130/140 [00:49<00:03,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  94%|█████████▎| 131/140 [00:50<00:03,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  94%|█████████▍| 132/140 [00:50<00:03,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  95%|█████████▌| 133/140 [00:51<00:02,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  96%|█████████▌| 134/140 [00:51<00:02,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  96%|█████████▋| 135/140 [00:51<00:01,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  97%|█████████▋| 136/140 [00:52<00:01,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  98%|█████████▊| 137/140 [00:52<00:01,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  99%|█████████▊| 138/140 [00:52<00:00,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  99%|█████████▉| 139/140 [00:53<00:00,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([30, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([30, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([30, 768])\n",
            "Final output shape: torch.Size([30, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 140/140 [00:53<00:00,  2.61it/s]\n",
            "Valid:   3%|▎         | 1/30 [00:00<00:07,  3.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  10%|█         | 3/30 [00:00<00:04,  6.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  17%|█▋        | 5/30 [00:00<00:03,  7.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  23%|██▎       | 7/30 [00:01<00:03,  6.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  30%|███       | 9/30 [00:01<00:02,  7.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  37%|███▋      | 11/30 [00:01<00:02,  7.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  43%|████▎     | 13/30 [00:01<00:02,  7.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  50%|█████     | 15/30 [00:02<00:01,  7.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  57%|█████▋    | 17/30 [00:02<00:01,  7.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  63%|██████▎   | 19/30 [00:02<00:01,  7.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  70%|███████   | 21/30 [00:02<00:01,  7.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  77%|███████▋  | 23/30 [00:03<00:00,  7.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  83%|████████▎ | 25/30 [00:03<00:00,  7.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  90%|█████████ | 27/30 [00:03<00:00,  7.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 30/30 [00:04<00:00,  7.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([10, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([10, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([10, 768])\n",
            "Final output shape: torch.Size([10, 768])\n",
            "Epoch 1: train_acc=0.492, val_acc=0.556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   0%|          | 0/140 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   1%|          | 1/140 [00:00<01:09,  2.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   1%|▏         | 2/140 [00:00<00:58,  2.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   2%|▏         | 3/140 [00:01<00:56,  2.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   3%|▎         | 4/140 [00:01<00:53,  2.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   4%|▎         | 5/140 [00:02<00:52,  2.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   4%|▍         | 6/140 [00:02<00:52,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   5%|▌         | 7/140 [00:02<00:50,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   6%|▌         | 8/140 [00:03<00:50,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   6%|▋         | 9/140 [00:03<00:50,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   7%|▋         | 10/140 [00:03<00:49,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   8%|▊         | 11/140 [00:04<00:49,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   9%|▊         | 12/140 [00:04<00:48,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   9%|▉         | 13/140 [00:05<00:48,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  10%|█         | 14/140 [00:05<00:48,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  11%|█         | 15/140 [00:05<00:47,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  11%|█▏        | 16/140 [00:06<00:47,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  12%|█▏        | 17/140 [00:06<00:46,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  13%|█▎        | 18/140 [00:06<00:46,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  14%|█▎        | 19/140 [00:07<00:45,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  14%|█▍        | 20/140 [00:07<00:45,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  15%|█▌        | 21/140 [00:08<00:45,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  16%|█▌        | 22/140 [00:08<00:44,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  16%|█▋        | 23/140 [00:08<00:44,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  17%|█▋        | 24/140 [00:09<00:43,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  18%|█▊        | 25/140 [00:09<00:43,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  19%|█▊        | 26/140 [00:09<00:43,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  19%|█▉        | 27/140 [00:10<00:42,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  20%|██        | 28/140 [00:10<00:42,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  21%|██        | 29/140 [00:11<00:42,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  21%|██▏       | 30/140 [00:11<00:41,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  22%|██▏       | 31/140 [00:11<00:41,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  23%|██▎       | 32/140 [00:12<00:41,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  24%|██▎       | 33/140 [00:12<00:40,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  24%|██▍       | 34/140 [00:13<00:40,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  25%|██▌       | 35/140 [00:13<00:39,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  26%|██▌       | 36/140 [00:13<00:39,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  26%|██▋       | 37/140 [00:14<00:39,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  27%|██▋       | 38/140 [00:14<00:38,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  28%|██▊       | 39/140 [00:14<00:38,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  29%|██▊       | 40/140 [00:15<00:37,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  29%|██▉       | 41/140 [00:15<00:37,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  30%|███       | 42/140 [00:16<00:37,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  31%|███       | 43/140 [00:16<00:36,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  31%|███▏      | 44/140 [00:16<00:36,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  32%|███▏      | 45/140 [00:17<00:36,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  33%|███▎      | 46/140 [00:17<00:35,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  34%|███▎      | 47/140 [00:17<00:35,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  34%|███▍      | 48/140 [00:18<00:34,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  35%|███▌      | 49/140 [00:18<00:34,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  36%|███▌      | 50/140 [00:19<00:33,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  36%|███▋      | 51/140 [00:19<00:33,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  37%|███▋      | 52/140 [00:19<00:33,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  38%|███▊      | 53/140 [00:20<00:32,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  39%|███▊      | 54/140 [00:20<00:32,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  39%|███▉      | 55/140 [00:21<00:32,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  40%|████      | 56/140 [00:21<00:31,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  41%|████      | 57/140 [00:21<00:31,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  41%|████▏     | 58/140 [00:22<00:31,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  42%|████▏     | 59/140 [00:22<00:30,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  43%|████▎     | 60/140 [00:22<00:30,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  44%|████▎     | 61/140 [00:23<00:29,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  44%|████▍     | 62/140 [00:23<00:29,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  45%|████▌     | 63/140 [00:24<00:29,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  46%|████▌     | 64/140 [00:24<00:28,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  46%|████▋     | 65/140 [00:24<00:28,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  47%|████▋     | 66/140 [00:25<00:28,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  48%|████▊     | 67/140 [00:25<00:27,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  49%|████▊     | 68/140 [00:25<00:27,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  49%|████▉     | 69/140 [00:26<00:27,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  50%|█████     | 70/140 [00:26<00:26,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  51%|█████     | 71/140 [00:27<00:26,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  51%|█████▏    | 72/140 [00:27<00:25,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  52%|█████▏    | 73/140 [00:27<00:25,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  53%|█████▎    | 74/140 [00:28<00:25,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  54%|█████▎    | 75/140 [00:28<00:24,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  54%|█████▍    | 76/140 [00:28<00:24,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  55%|█████▌    | 77/140 [00:29<00:24,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  56%|█████▌    | 78/140 [00:29<00:23,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  56%|█████▋    | 79/140 [00:30<00:23,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  57%|█████▋    | 80/140 [00:30<00:22,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  58%|█████▊    | 81/140 [00:30<00:22,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  59%|█████▊    | 82/140 [00:31<00:22,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  59%|█████▉    | 83/140 [00:31<00:21,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  60%|██████    | 84/140 [00:32<00:21,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  61%|██████    | 85/140 [00:32<00:20,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  61%|██████▏   | 86/140 [00:32<00:20,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  62%|██████▏   | 87/140 [00:33<00:20,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  63%|██████▎   | 88/140 [00:33<00:19,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  64%|██████▎   | 89/140 [00:33<00:19,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  64%|██████▍   | 90/140 [00:34<00:18,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  65%|██████▌   | 91/140 [00:34<00:18,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  66%|██████▌   | 92/140 [00:35<00:18,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  66%|██████▋   | 93/140 [00:35<00:17,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  67%|██████▋   | 94/140 [00:35<00:17,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  68%|██████▊   | 95/140 [00:36<00:17,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  69%|██████▊   | 96/140 [00:36<00:16,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  69%|██████▉   | 97/140 [00:36<00:16,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  70%|███████   | 98/140 [00:37<00:15,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  71%|███████   | 99/140 [00:37<00:15,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  71%|███████▏  | 100/140 [00:38<00:15,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  72%|███████▏  | 101/140 [00:38<00:14,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  73%|███████▎  | 102/140 [00:38<00:14,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  74%|███████▎  | 103/140 [00:39<00:14,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  74%|███████▍  | 104/140 [00:39<00:13,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  75%|███████▌  | 105/140 [00:40<00:13,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  76%|███████▌  | 106/140 [00:40<00:12,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  76%|███████▋  | 107/140 [00:40<00:12,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  77%|███████▋  | 108/140 [00:41<00:12,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  78%|███████▊  | 109/140 [00:41<00:11,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  79%|███████▊  | 110/140 [00:41<00:11,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  79%|███████▉  | 111/140 [00:42<00:11,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  80%|████████  | 112/140 [00:42<00:10,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  81%|████████  | 113/140 [00:43<00:10,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  81%|████████▏ | 114/140 [00:43<00:09,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  82%|████████▏ | 115/140 [00:43<00:09,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  83%|████████▎ | 116/140 [00:44<00:09,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  84%|████████▎ | 117/140 [00:44<00:08,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  84%|████████▍ | 118/140 [00:44<00:08,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  85%|████████▌ | 119/140 [00:45<00:08,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  86%|████████▌ | 120/140 [00:45<00:07,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  86%|████████▋ | 121/140 [00:46<00:07,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  87%|████████▋ | 122/140 [00:46<00:06,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  88%|████████▊ | 123/140 [00:46<00:06,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  89%|████████▊ | 124/140 [00:47<00:06,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  89%|████████▉ | 125/140 [00:47<00:05,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  90%|█████████ | 126/140 [00:48<00:05,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  91%|█████████ | 127/140 [00:48<00:04,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  91%|█████████▏| 128/140 [00:48<00:04,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  92%|█████████▏| 129/140 [00:49<00:04,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  93%|█████████▎| 130/140 [00:49<00:03,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  94%|█████████▎| 131/140 [00:49<00:03,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  94%|█████████▍| 132/140 [00:50<00:03,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  95%|█████████▌| 133/140 [00:50<00:02,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  96%|█████████▌| 134/140 [00:51<00:02,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  96%|█████████▋| 135/140 [00:51<00:01,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  97%|█████████▋| 136/140 [00:51<00:01,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  98%|█████████▊| 137/140 [00:52<00:01,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  99%|█████████▊| 138/140 [00:52<00:00,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  99%|█████████▉| 139/140 [00:52<00:00,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([30, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([30, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([30, 768])\n",
            "Final output shape: torch.Size([30, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 140/140 [00:53<00:00,  2.62it/s]\n",
            "Valid:   3%|▎         | 1/30 [00:00<00:07,  3.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  10%|█         | 3/30 [00:00<00:04,  6.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  17%|█▋        | 5/30 [00:00<00:03,  7.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  23%|██▎       | 7/30 [00:01<00:03,  7.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  30%|███       | 9/30 [00:01<00:02,  7.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  37%|███▋      | 11/30 [00:01<00:02,  7.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  43%|████▎     | 13/30 [00:01<00:02,  7.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  50%|█████     | 15/30 [00:02<00:01,  7.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  57%|█████▋    | 17/30 [00:02<00:01,  7.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  63%|██████▎   | 19/30 [00:02<00:01,  7.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  70%|███████   | 21/30 [00:02<00:01,  7.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  77%|███████▋  | 23/30 [00:03<00:00,  7.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  83%|████████▎ | 25/30 [00:03<00:00,  7.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  90%|█████████ | 27/30 [00:03<00:00,  7.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 30/30 [00:04<00:00,  7.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([10, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([10, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([10, 768])\n",
            "Final output shape: torch.Size([10, 768])\n",
            "Epoch 2: train_acc=0.483, val_acc=0.556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   0%|          | 0/140 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   1%|          | 1/140 [00:00<01:09,  1.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   1%|▏         | 2/140 [00:00<00:58,  2.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   2%|▏         | 3/140 [00:01<00:56,  2.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   3%|▎         | 4/140 [00:01<00:54,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   4%|▎         | 5/140 [00:02<00:53,  2.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   4%|▍         | 6/140 [00:02<00:52,  2.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   5%|▌         | 7/140 [00:02<00:51,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   6%|▌         | 8/140 [00:03<00:51,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   6%|▋         | 9/140 [00:03<00:50,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   7%|▋         | 10/140 [00:03<00:49,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   8%|▊         | 11/140 [00:04<00:49,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   9%|▊         | 12/140 [00:04<00:49,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   9%|▉         | 13/140 [00:05<00:48,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  10%|█         | 14/140 [00:05<00:48,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  11%|█         | 15/140 [00:05<00:47,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  11%|█▏        | 16/140 [00:06<00:47,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  12%|█▏        | 17/140 [00:06<00:47,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  13%|█▎        | 18/140 [00:07<00:46,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  14%|█▎        | 19/140 [00:07<00:46,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  14%|█▍        | 20/140 [00:07<00:45,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  15%|█▌        | 21/140 [00:08<00:45,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  16%|█▌        | 22/140 [00:08<00:45,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  16%|█▋        | 23/140 [00:08<00:44,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  17%|█▋        | 24/140 [00:09<00:44,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  18%|█▊        | 25/140 [00:09<00:43,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  19%|█▊        | 26/140 [00:10<00:43,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  19%|█▉        | 27/140 [00:10<00:43,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  20%|██        | 28/140 [00:10<00:42,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  21%|██        | 29/140 [00:11<00:42,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  21%|██▏       | 30/140 [00:11<00:41,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  22%|██▏       | 31/140 [00:11<00:41,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  23%|██▎       | 32/140 [00:12<00:41,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  24%|██▎       | 33/140 [00:12<00:40,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  24%|██▍       | 34/140 [00:13<00:40,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  25%|██▌       | 35/140 [00:13<00:40,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  26%|██▌       | 36/140 [00:13<00:39,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  26%|██▋       | 37/140 [00:14<00:39,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  27%|██▋       | 38/140 [00:14<00:38,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  28%|██▊       | 39/140 [00:15<00:38,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  29%|██▊       | 40/140 [00:15<00:38,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  29%|██▉       | 41/140 [00:15<00:37,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  30%|███       | 42/140 [00:16<00:37,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  31%|███       | 43/140 [00:16<00:36,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  31%|███▏      | 44/140 [00:16<00:36,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  32%|███▏      | 45/140 [00:17<00:36,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  33%|███▎      | 46/140 [00:17<00:35,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  34%|███▎      | 47/140 [00:18<00:35,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  34%|███▍      | 48/140 [00:18<00:35,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  35%|███▌      | 49/140 [00:18<00:34,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  36%|███▌      | 50/140 [00:19<00:34,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  36%|███▋      | 51/140 [00:19<00:34,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  37%|███▋      | 52/140 [00:19<00:33,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  38%|███▊      | 53/140 [00:20<00:33,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  39%|███▊      | 54/140 [00:20<00:32,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  39%|███▉      | 55/140 [00:21<00:32,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  40%|████      | 56/140 [00:21<00:32,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  41%|████      | 57/140 [00:21<00:31,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  41%|████▏     | 58/140 [00:22<00:31,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  42%|████▏     | 59/140 [00:22<00:31,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  43%|████▎     | 60/140 [00:23<00:30,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  44%|████▎     | 61/140 [00:23<00:30,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  44%|████▍     | 62/140 [00:23<00:29,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  45%|████▌     | 63/140 [00:24<00:29,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  46%|████▌     | 64/140 [00:24<00:28,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  46%|████▋     | 65/140 [00:24<00:28,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  47%|████▋     | 66/140 [00:25<00:28,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  48%|████▊     | 67/140 [00:25<00:27,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  49%|████▊     | 68/140 [00:26<00:27,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  49%|████▉     | 69/140 [00:26<00:27,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  50%|█████     | 70/140 [00:26<00:26,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  51%|█████     | 71/140 [00:27<00:26,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  51%|█████▏    | 72/140 [00:27<00:26,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  52%|█████▏    | 73/140 [00:27<00:25,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  53%|█████▎    | 74/140 [00:28<00:25,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  54%|█████▎    | 75/140 [00:28<00:24,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  54%|█████▍    | 76/140 [00:29<00:24,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  55%|█████▌    | 77/140 [00:29<00:24,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  56%|█████▌    | 78/140 [00:29<00:23,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  56%|█████▋    | 79/140 [00:30<00:23,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  57%|█████▋    | 80/140 [00:30<00:22,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  58%|█████▊    | 81/140 [00:31<00:22,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  59%|█████▊    | 82/140 [00:31<00:22,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  59%|█████▉    | 83/140 [00:31<00:21,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  60%|██████    | 84/140 [00:32<00:21,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  61%|██████    | 85/140 [00:32<00:21,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  61%|██████▏   | 86/140 [00:32<00:20,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  62%|██████▏   | 87/140 [00:33<00:20,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  63%|██████▎   | 88/140 [00:33<00:19,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  64%|██████▎   | 89/140 [00:34<00:19,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  64%|██████▍   | 90/140 [00:34<00:19,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  65%|██████▌   | 91/140 [00:34<00:18,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  66%|██████▌   | 92/140 [00:35<00:18,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  66%|██████▋   | 93/140 [00:35<00:17,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  67%|██████▋   | 94/140 [00:35<00:17,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  68%|██████▊   | 95/140 [00:36<00:17,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  69%|██████▊   | 96/140 [00:36<00:16,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  69%|██████▉   | 97/140 [00:37<00:16,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  70%|███████   | 98/140 [00:37<00:16,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  71%|███████   | 99/140 [00:37<00:15,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  71%|███████▏  | 100/140 [00:38<00:15,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  72%|███████▏  | 101/140 [00:38<00:14,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  73%|███████▎  | 102/140 [00:39<00:14,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  74%|███████▎  | 103/140 [00:39<00:14,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  74%|███████▍  | 104/140 [00:39<00:13,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  75%|███████▌  | 105/140 [00:40<00:13,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  76%|███████▌  | 106/140 [00:40<00:13,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  76%|███████▋  | 107/140 [00:40<00:12,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  77%|███████▋  | 108/140 [00:41<00:12,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  78%|███████▊  | 109/140 [00:41<00:11,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  79%|███████▊  | 110/140 [00:42<00:11,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  79%|███████▉  | 111/140 [00:42<00:11,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  80%|████████  | 112/140 [00:42<00:10,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  81%|████████  | 113/140 [00:43<00:10,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  81%|████████▏ | 114/140 [00:43<00:10,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  82%|████████▏ | 115/140 [00:44<00:09,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  83%|████████▎ | 116/140 [00:44<00:09,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  84%|████████▎ | 117/140 [00:44<00:08,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  84%|████████▍ | 118/140 [00:45<00:08,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  85%|████████▌ | 119/140 [00:45<00:08,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  86%|████████▌ | 120/140 [00:45<00:07,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  86%|████████▋ | 121/140 [00:46<00:07,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  87%|████████▋ | 122/140 [00:46<00:06,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  88%|████████▊ | 123/140 [00:47<00:06,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  89%|████████▊ | 124/140 [00:47<00:06,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  89%|████████▉ | 125/140 [00:47<00:05,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  90%|█████████ | 126/140 [00:48<00:05,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  91%|█████████ | 127/140 [00:48<00:05,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  91%|█████████▏| 128/140 [00:49<00:04,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  92%|█████████▏| 129/140 [00:49<00:04,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  93%|█████████▎| 130/140 [00:49<00:03,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  94%|█████████▎| 131/140 [00:50<00:03,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  94%|█████████▍| 132/140 [00:50<00:03,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  95%|█████████▌| 133/140 [00:50<00:02,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  96%|█████████▌| 134/140 [00:51<00:02,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  96%|█████████▋| 135/140 [00:51<00:01,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  97%|█████████▋| 136/140 [00:52<00:01,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  98%|█████████▊| 137/140 [00:52<00:01,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  99%|█████████▊| 138/140 [00:52<00:00,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  99%|█████████▉| 139/140 [00:53<00:00,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([30, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([30, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([30, 768])\n",
            "Final output shape: torch.Size([30, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 140/140 [00:53<00:00,  2.61it/s]\n",
            "Valid:   3%|▎         | 1/30 [00:00<00:07,  3.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  10%|█         | 3/30 [00:00<00:04,  6.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  17%|█▋        | 5/30 [00:00<00:03,  6.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  23%|██▎       | 7/30 [00:01<00:03,  6.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  30%|███       | 9/30 [00:01<00:02,  7.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  37%|███▋      | 11/30 [00:01<00:02,  7.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  43%|████▎     | 13/30 [00:01<00:02,  7.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  50%|█████     | 15/30 [00:02<00:02,  7.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  57%|█████▋    | 17/30 [00:02<00:01,  7.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  63%|██████▎   | 19/30 [00:02<00:01,  7.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  70%|███████   | 21/30 [00:02<00:01,  7.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  77%|███████▋  | 23/30 [00:03<00:00,  7.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  83%|████████▎ | 25/30 [00:03<00:00,  7.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  90%|█████████ | 27/30 [00:03<00:00,  7.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 30/30 [00:04<00:00,  7.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([10, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([10, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([10, 768])\n",
            "Final output shape: torch.Size([10, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: train_acc=0.511, val_acc=0.556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   0%|          | 0/140 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   1%|          | 1/140 [00:00<01:10,  1.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   1%|▏         | 2/140 [00:00<00:58,  2.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   2%|▏         | 3/140 [00:01<00:57,  2.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   3%|▎         | 4/140 [00:01<00:54,  2.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   4%|▎         | 5/140 [00:02<00:53,  2.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   4%|▍         | 6/140 [00:02<00:52,  2.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   5%|▌         | 7/140 [00:02<00:51,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   6%|▌         | 8/140 [00:03<00:51,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   6%|▋         | 9/140 [00:03<00:50,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   7%|▋         | 10/140 [00:03<00:49,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   8%|▊         | 11/140 [00:04<00:49,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   9%|▊         | 12/140 [00:04<00:49,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   9%|▉         | 13/140 [00:05<00:48,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  10%|█         | 14/140 [00:05<00:48,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  11%|█         | 15/140 [00:05<00:47,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  11%|█▏        | 16/140 [00:06<00:47,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  12%|█▏        | 17/140 [00:06<00:47,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  13%|█▎        | 18/140 [00:07<00:46,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  14%|█▎        | 19/140 [00:07<00:46,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  14%|█▍        | 20/140 [00:07<00:45,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  15%|█▌        | 21/140 [00:08<00:45,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  16%|█▌        | 22/140 [00:08<00:45,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  16%|█▋        | 23/140 [00:08<00:44,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  17%|█▋        | 24/140 [00:09<00:44,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  18%|█▊        | 25/140 [00:09<00:44,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  19%|█▊        | 26/140 [00:10<00:43,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  19%|█▉        | 27/140 [00:10<00:43,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  20%|██        | 28/140 [00:10<00:42,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  21%|██        | 29/140 [00:11<00:42,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  21%|██▏       | 30/140 [00:11<00:42,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  22%|██▏       | 31/140 [00:12<00:41,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  23%|██▎       | 32/140 [00:12<00:41,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  24%|██▎       | 33/140 [00:12<00:41,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  24%|██▍       | 34/140 [00:13<00:40,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  25%|██▌       | 35/140 [00:13<00:40,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  26%|██▌       | 36/140 [00:13<00:40,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  26%|██▋       | 37/140 [00:14<00:39,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  27%|██▋       | 38/140 [00:14<00:39,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  28%|██▊       | 39/140 [00:15<00:38,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  29%|██▊       | 40/140 [00:15<00:38,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  29%|██▉       | 41/140 [00:15<00:37,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  30%|███       | 42/140 [00:16<00:37,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  31%|███       | 43/140 [00:16<00:37,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  31%|███▏      | 44/140 [00:17<00:36,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  32%|███▏      | 45/140 [00:17<00:36,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  33%|███▎      | 46/140 [00:17<00:36,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  34%|███▎      | 47/140 [00:18<00:35,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  34%|███▍      | 48/140 [00:18<00:35,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  35%|███▌      | 49/140 [00:18<00:34,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  36%|███▌      | 50/140 [00:19<00:34,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  36%|███▋      | 51/140 [00:19<00:34,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  37%|███▋      | 52/140 [00:20<00:33,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  38%|███▊      | 53/140 [00:20<00:33,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  39%|███▊      | 54/140 [00:20<00:33,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  39%|███▉      | 55/140 [00:21<00:32,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  40%|████      | 56/140 [00:21<00:32,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  41%|████      | 57/140 [00:22<00:32,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  41%|████▏     | 58/140 [00:22<00:31,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  42%|████▏     | 59/140 [00:22<00:31,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  43%|████▎     | 60/140 [00:23<00:30,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  44%|████▎     | 61/140 [00:23<00:30,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  44%|████▍     | 62/140 [00:23<00:29,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  45%|████▌     | 63/140 [00:24<00:29,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  46%|████▌     | 64/140 [00:24<00:29,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  46%|████▋     | 65/140 [00:25<00:28,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  47%|████▋     | 66/140 [00:25<00:28,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  48%|████▊     | 67/140 [00:25<00:28,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  49%|████▊     | 68/140 [00:26<00:27,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  49%|████▉     | 69/140 [00:26<00:27,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  50%|█████     | 70/140 [00:27<00:26,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  51%|█████     | 71/140 [00:27<00:26,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  51%|█████▏    | 72/140 [00:27<00:26,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  52%|█████▏    | 73/140 [00:28<00:25,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  53%|█████▎    | 74/140 [00:28<00:25,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  54%|█████▎    | 75/140 [00:28<00:24,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  54%|█████▍    | 76/140 [00:29<00:24,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  55%|█████▌    | 77/140 [00:29<00:24,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  56%|█████▌    | 78/140 [00:30<00:23,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  56%|█████▋    | 79/140 [00:30<00:23,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  57%|█████▋    | 80/140 [00:30<00:23,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  58%|█████▊    | 81/140 [00:31<00:22,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  59%|█████▊    | 82/140 [00:31<00:22,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  59%|█████▉    | 83/140 [00:32<00:21,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  60%|██████    | 84/140 [00:32<00:21,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  61%|██████    | 85/140 [00:32<00:21,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  61%|██████▏   | 86/140 [00:33<00:20,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  62%|██████▏   | 87/140 [00:33<00:20,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  63%|██████▎   | 88/140 [00:33<00:19,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  64%|██████▎   | 89/140 [00:34<00:19,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  64%|██████▍   | 90/140 [00:34<00:19,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  65%|██████▌   | 91/140 [00:35<00:18,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  66%|██████▌   | 92/140 [00:35<00:18,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  66%|██████▋   | 93/140 [00:35<00:18,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  67%|██████▋   | 94/140 [00:36<00:17,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  68%|██████▊   | 95/140 [00:36<00:17,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  69%|██████▊   | 96/140 [00:36<00:16,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  69%|██████▉   | 97/140 [00:37<00:16,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  70%|███████   | 98/140 [00:37<00:16,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  71%|███████   | 99/140 [00:38<00:15,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  71%|███████▏  | 100/140 [00:38<00:15,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  72%|███████▏  | 101/140 [00:38<00:14,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  73%|███████▎  | 102/140 [00:39<00:14,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  74%|███████▎  | 103/140 [00:39<00:14,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  74%|███████▍  | 104/140 [00:40<00:13,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  75%|███████▌  | 105/140 [00:40<00:13,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  76%|███████▌  | 106/140 [00:40<00:13,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  76%|███████▋  | 107/140 [00:41<00:12,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  77%|███████▋  | 108/140 [00:41<00:12,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  78%|███████▊  | 109/140 [00:41<00:11,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  79%|███████▊  | 110/140 [00:42<00:11,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  79%|███████▉  | 111/140 [00:42<00:11,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  80%|████████  | 112/140 [00:43<00:10,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  81%|████████  | 113/140 [00:43<00:10,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  81%|████████▏ | 114/140 [00:43<00:09,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  82%|████████▏ | 115/140 [00:44<00:09,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  83%|████████▎ | 116/140 [00:44<00:09,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  84%|████████▎ | 117/140 [00:45<00:08,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  84%|████████▍ | 118/140 [00:45<00:08,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  85%|████████▌ | 119/140 [00:45<00:08,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  86%|████████▌ | 120/140 [00:46<00:07,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  86%|████████▋ | 121/140 [00:46<00:07,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  87%|████████▋ | 122/140 [00:46<00:06,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  88%|████████▊ | 123/140 [00:47<00:06,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  89%|████████▊ | 124/140 [00:47<00:06,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  89%|████████▉ | 125/140 [00:48<00:05,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  90%|█████████ | 126/140 [00:48<00:05,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  91%|█████████ | 127/140 [00:48<00:04,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  91%|█████████▏| 128/140 [00:49<00:04,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  92%|█████████▏| 129/140 [00:49<00:04,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  93%|█████████▎| 130/140 [00:50<00:03,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  94%|█████████▎| 131/140 [00:50<00:03,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  94%|█████████▍| 132/140 [00:50<00:03,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  95%|█████████▌| 133/140 [00:51<00:02,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  96%|█████████▌| 134/140 [00:51<00:02,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  96%|█████████▋| 135/140 [00:51<00:01,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  97%|█████████▋| 136/140 [00:52<00:01,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  98%|█████████▊| 137/140 [00:52<00:01,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  99%|█████████▊| 138/140 [00:53<00:00,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  99%|█████████▉| 139/140 [00:53<00:00,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([30, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([30, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([30, 768])\n",
            "Final output shape: torch.Size([30, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 140/140 [00:53<00:00,  2.60it/s]\n",
            "Valid:   3%|▎         | 1/30 [00:00<00:07,  3.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  10%|█         | 3/30 [00:00<00:04,  6.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  17%|█▋        | 5/30 [00:00<00:03,  6.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  23%|██▎       | 7/30 [00:01<00:03,  7.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  30%|███       | 9/30 [00:01<00:02,  7.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  37%|███▋      | 11/30 [00:01<00:02,  7.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  43%|████▎     | 13/30 [00:01<00:02,  7.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  50%|█████     | 15/30 [00:02<00:01,  7.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  57%|█████▋    | 17/30 [00:02<00:01,  7.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  63%|██████▎   | 19/30 [00:02<00:01,  7.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  70%|███████   | 21/30 [00:02<00:01,  7.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  77%|███████▋  | 23/30 [00:03<00:00,  7.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  83%|████████▎ | 25/30 [00:03<00:00,  7.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  90%|█████████ | 27/30 [00:03<00:00,  7.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 30/30 [00:04<00:00,  7.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([10, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([10, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([10, 768])\n",
            "Final output shape: torch.Size([10, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: train_acc=0.592, val_acc=0.444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   0%|          | 0/140 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   1%|          | 1/140 [00:00<01:11,  1.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   1%|▏         | 2/140 [00:00<00:59,  2.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   2%|▏         | 3/140 [00:01<00:57,  2.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   3%|▎         | 4/140 [00:01<00:54,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   4%|▎         | 5/140 [00:02<00:53,  2.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   4%|▍         | 6/140 [00:02<00:52,  2.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   5%|▌         | 7/140 [00:02<00:51,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   6%|▌         | 8/140 [00:03<00:50,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   6%|▋         | 9/140 [00:03<00:50,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   7%|▋         | 10/140 [00:03<00:49,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   8%|▊         | 11/140 [00:04<00:49,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   9%|▊         | 12/140 [00:04<00:49,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   9%|▉         | 13/140 [00:05<00:48,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  10%|█         | 14/140 [00:05<00:48,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  11%|█         | 15/140 [00:05<00:47,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  11%|█▏        | 16/140 [00:06<00:47,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  12%|█▏        | 17/140 [00:06<00:47,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  13%|█▎        | 18/140 [00:07<00:46,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  14%|█▎        | 19/140 [00:07<00:46,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  14%|█▍        | 20/140 [00:07<00:45,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  15%|█▌        | 21/140 [00:08<00:45,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  16%|█▌        | 22/140 [00:08<00:45,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  16%|█▋        | 23/140 [00:08<00:44,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  17%|█▋        | 24/140 [00:09<00:44,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  18%|█▊        | 25/140 [00:09<00:43,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  19%|█▊        | 26/140 [00:10<00:43,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  19%|█▉        | 27/140 [00:10<00:43,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  20%|██        | 28/140 [00:10<00:42,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  21%|██        | 29/140 [00:11<00:42,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  21%|██▏       | 30/140 [00:11<00:42,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  22%|██▏       | 31/140 [00:11<00:41,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  23%|██▎       | 32/140 [00:12<00:41,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  24%|██▎       | 33/140 [00:12<00:41,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  24%|██▍       | 34/140 [00:13<00:40,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  25%|██▌       | 35/140 [00:13<00:40,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  26%|██▌       | 36/140 [00:13<00:39,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  26%|██▋       | 37/140 [00:14<00:39,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  27%|██▋       | 38/140 [00:14<00:39,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  28%|██▊       | 39/140 [00:15<00:38,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  29%|██▊       | 40/140 [00:15<00:38,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  29%|██▉       | 41/140 [00:15<00:38,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  30%|███       | 42/140 [00:16<00:37,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  31%|███       | 43/140 [00:16<00:37,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  31%|███▏      | 44/140 [00:16<00:36,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  32%|███▏      | 45/140 [00:17<00:36,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  33%|███▎      | 46/140 [00:17<00:36,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  34%|███▎      | 47/140 [00:18<00:35,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  34%|███▍      | 48/140 [00:18<00:35,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  35%|███▌      | 49/140 [00:18<00:34,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  36%|███▌      | 50/140 [00:19<00:34,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  36%|███▋      | 51/140 [00:19<00:34,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  37%|███▋      | 52/140 [00:20<00:33,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  38%|███▊      | 53/140 [00:20<00:33,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  39%|███▊      | 54/140 [00:20<00:32,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  39%|███▉      | 55/140 [00:21<00:32,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  40%|████      | 56/140 [00:21<00:32,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  41%|████      | 57/140 [00:21<00:31,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  41%|████▏     | 58/140 [00:22<00:31,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  42%|████▏     | 59/140 [00:22<00:30,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  43%|████▎     | 60/140 [00:23<00:30,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  44%|████▎     | 61/140 [00:23<00:30,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  44%|████▍     | 62/140 [00:23<00:29,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  45%|████▌     | 63/140 [00:24<00:29,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  46%|████▌     | 64/140 [00:24<00:29,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  46%|████▋     | 65/140 [00:25<00:28,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  47%|████▋     | 66/140 [00:25<00:28,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  48%|████▊     | 67/140 [00:25<00:28,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  49%|████▊     | 68/140 [00:26<00:27,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  49%|████▉     | 69/140 [00:26<00:27,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  50%|█████     | 70/140 [00:26<00:26,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  51%|█████     | 71/140 [00:27<00:26,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  51%|█████▏    | 72/140 [00:27<00:26,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  52%|█████▏    | 73/140 [00:28<00:25,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  53%|█████▎    | 74/140 [00:28<00:25,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  54%|█████▎    | 75/140 [00:28<00:24,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  54%|█████▍    | 76/140 [00:29<00:24,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  55%|█████▌    | 77/140 [00:29<00:24,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  56%|█████▌    | 78/140 [00:30<00:23,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  56%|█████▋    | 79/140 [00:30<00:23,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  57%|█████▋    | 80/140 [00:30<00:23,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  58%|█████▊    | 81/140 [00:31<00:22,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  59%|█████▊    | 82/140 [00:31<00:22,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  59%|█████▉    | 83/140 [00:31<00:21,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  60%|██████    | 84/140 [00:32<00:21,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  61%|██████    | 85/140 [00:32<00:21,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  61%|██████▏   | 86/140 [00:33<00:20,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  62%|██████▏   | 87/140 [00:33<00:20,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  63%|██████▎   | 88/140 [00:33<00:19,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  64%|██████▎   | 89/140 [00:34<00:19,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  64%|██████▍   | 90/140 [00:34<00:19,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  65%|██████▌   | 91/140 [00:34<00:18,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  66%|██████▌   | 92/140 [00:35<00:18,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  66%|██████▋   | 93/140 [00:35<00:18,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  67%|██████▋   | 94/140 [00:36<00:17,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  68%|██████▊   | 95/140 [00:36<00:17,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  69%|██████▊   | 96/140 [00:36<00:16,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  69%|██████▉   | 97/140 [00:37<00:16,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  70%|███████   | 98/140 [00:37<00:16,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  71%|███████   | 99/140 [00:38<00:15,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  71%|███████▏  | 100/140 [00:38<00:15,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  72%|███████▏  | 101/140 [00:38<00:15,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  73%|███████▎  | 102/140 [00:39<00:14,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  74%|███████▎  | 103/140 [00:39<00:14,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  74%|███████▍  | 104/140 [00:39<00:13,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  75%|███████▌  | 105/140 [00:40<00:13,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  76%|███████▌  | 106/140 [00:40<00:13,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  76%|███████▋  | 107/140 [00:41<00:12,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  77%|███████▋  | 108/140 [00:41<00:12,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  78%|███████▊  | 109/140 [00:41<00:11,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  79%|███████▊  | 110/140 [00:42<00:11,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  79%|███████▉  | 111/140 [00:42<00:11,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  80%|████████  | 112/140 [00:43<00:10,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  81%|████████  | 113/140 [00:43<00:10,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  81%|████████▏ | 114/140 [00:43<00:10,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  82%|████████▏ | 115/140 [00:44<00:09,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  83%|████████▎ | 116/140 [00:44<00:09,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  84%|████████▎ | 117/140 [00:44<00:08,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  84%|████████▍ | 118/140 [00:45<00:08,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  85%|████████▌ | 119/140 [00:45<00:08,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  86%|████████▌ | 120/140 [00:46<00:07,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  86%|████████▋ | 121/140 [00:46<00:07,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  87%|████████▋ | 122/140 [00:46<00:06,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  88%|████████▊ | 123/140 [00:47<00:06,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  89%|████████▊ | 124/140 [00:47<00:06,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  89%|████████▉ | 125/140 [00:48<00:05,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  90%|█████████ | 126/140 [00:48<00:05,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  91%|█████████ | 127/140 [00:48<00:05,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  91%|█████████▏| 128/140 [00:49<00:04,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  92%|█████████▏| 129/140 [00:49<00:04,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  93%|█████████▎| 130/140 [00:49<00:03,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  94%|█████████▎| 131/140 [00:50<00:03,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  94%|█████████▍| 132/140 [00:50<00:03,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  95%|█████████▌| 133/140 [00:51<00:02,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  96%|█████████▌| 134/140 [00:51<00:02,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  96%|█████████▋| 135/140 [00:51<00:01,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  97%|█████████▋| 136/140 [00:52<00:01,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  98%|█████████▊| 137/140 [00:52<00:01,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  99%|█████████▊| 138/140 [00:53<00:00,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  99%|█████████▉| 139/140 [00:53<00:00,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([30, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([30, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([30, 768])\n",
            "Final output shape: torch.Size([30, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 140/140 [00:53<00:00,  2.60it/s]\n",
            "Valid:   3%|▎         | 1/30 [00:00<00:07,  3.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  10%|█         | 3/30 [00:00<00:04,  6.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  17%|█▋        | 5/30 [00:00<00:03,  7.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  23%|██▎       | 7/30 [00:01<00:03,  6.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  30%|███       | 9/30 [00:01<00:02,  7.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  37%|███▋      | 11/30 [00:01<00:02,  7.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  43%|████▎     | 13/30 [00:01<00:02,  7.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  50%|█████     | 15/30 [00:02<00:02,  7.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  57%|█████▋    | 17/30 [00:02<00:01,  7.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  63%|██████▎   | 19/30 [00:02<00:01,  7.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  70%|███████   | 21/30 [00:02<00:01,  7.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  77%|███████▋  | 23/30 [00:03<00:00,  7.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  83%|████████▎ | 25/30 [00:03<00:00,  7.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid:  90%|█████████ | 27/30 [00:03<00:00,  7.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 30/30 [00:04<00:00,  7.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Input shape: torch.Size([40, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([40, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([40, 768])\n",
            "Final output shape: torch.Size([40, 768])\n",
            "CLIP Input shape: torch.Size([10, 3, 224, 224])\n",
            "Feeding to CLIP: torch.Size([10, 3, 224, 224])\n",
            "CLIP features shape: torch.Size([10, 768])\n",
            "Final output shape: torch.Size([10, 768])\n",
            "Epoch 5: train_acc=0.716, val_acc=0.406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to trained_models/dino2d_attention_epochs_5_model.pth\n",
            "\n",
            "--- Processing backbone: resnet18, temporal pooling: mean ---\n",
            "Shape of data before calling get_train_val_test_loaders: (62, 80, 1, 100, 100)\n",
            "Length of train_idx: 43\n",
            "get_train_val_test_loaders: Shape of data and labels: (62, 80, 1, 100, 100), (62,)\n",
            "get_train_val_test_loaders: Length of train_idx: 43\n",
            "Training new model for backbone: resnet18, temporal pooling: mean\n",
            "####### auto\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 219MB/s]\n",
            "Train: 100%|██████████| 140/140 [00:19<00:00,  7.05it/s]\n",
            "Valid: 100%|██████████| 30/30 [00:01<00:00, 19.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train_acc=0.898, val_acc=0.444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 140/140 [00:19<00:00,  7.17it/s]\n",
            "Valid: 100%|██████████| 30/30 [00:01<00:00, 19.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: train_acc=0.946, val_acc=0.970\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 140/140 [00:19<00:00,  7.18it/s]\n",
            "Valid: 100%|██████████| 30/30 [00:01<00:00, 19.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: train_acc=0.941, val_acc=0.970\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 140/140 [00:19<00:00,  7.19it/s]\n",
            "Valid: 100%|██████████| 30/30 [00:01<00:00, 19.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: train_acc=0.958, val_acc=0.979\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 140/140 [00:19<00:00,  7.20it/s]\n",
            "Valid: 100%|██████████| 30/30 [00:01<00:00, 19.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: train_acc=0.970, val_acc=0.940\n",
            "Model saved to trained_models/resnet18_mean_epochs_5_model.pth\n",
            "\n",
            "--- Processing backbone: resnet18, temporal pooling: attention ---\n",
            "Shape of data before calling get_train_val_test_loaders: (62, 80, 1, 100, 100)\n",
            "Length of train_idx: 43\n",
            "get_train_val_test_loaders: Shape of data and labels: (62, 80, 1, 100, 100), (62,)\n",
            "get_train_val_test_loaders: Length of train_idx: 43\n",
            "Training new model for backbone: resnet18, temporal pooling: attention\n",
            "####### auto\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 140/140 [00:19<00:00,  7.17it/s]\n",
            "Valid: 100%|██████████| 30/30 [00:01<00:00, 19.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train_acc=0.898, val_acc=0.919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 140/140 [00:19<00:00,  7.18it/s]\n",
            "Valid: 100%|██████████| 30/30 [00:01<00:00, 18.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: train_acc=0.939, val_acc=0.970\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 140/140 [00:19<00:00,  7.16it/s]\n",
            "Valid: 100%|██████████| 30/30 [00:01<00:00, 18.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: train_acc=0.936, val_acc=0.974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 140/140 [00:19<00:00,  7.16it/s]\n",
            "Valid: 100%|██████████| 30/30 [00:01<00:00, 19.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: train_acc=0.945, val_acc=0.944\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 140/140 [00:19<00:00,  7.18it/s]\n",
            "Valid: 100%|██████████| 30/30 [00:01<00:00, 19.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: train_acc=0.962, val_acc=0.983\n",
            "Model saved to trained_models/resnet18_attention_epochs_5_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> TEST MODEL"
      ],
      "metadata": {
        "id": "6dYhXua-INtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "# Assuming VideoClassifier and build_pooling are available from imported modules\n",
        "from models.classifier import VideoClassifier # Ensure this import is present\n",
        "\n",
        "def evaluate_and_save_model_results(model, test_loader, device, backbone_name, temporal_pooling_method, num_epochs, results_dir):\n",
        "    \"\"\"Evaluates a model and saves the results.\"\"\"\n",
        "    print(f\"\\nEvaluating model: Backbone={backbone_name}, Pooling={temporal_pooling_method}, Epochs={num_epochs}\")\n",
        "\n",
        "    # Perform evaluation\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for clips, labels in tqdm(test_loader, desc=f\"Evaluating {backbone_name}-{temporal_pooling_method}-Epochs{num_epochs}\"):\n",
        "            B, T, C, H, W = clips.shape\n",
        "            clips, labels = clips.to(device), labels.to(device)\n",
        "\n",
        "            clips = clips.view(B*T, C, H, W)\n",
        "            if clips.shape[1] == 1:\n",
        "                 clips = clips.repeat(1, 3, 1, 1)\n",
        "\n",
        "            logits = model(clips, B, T)\n",
        "            probs = torch.softmax(logits, dim=1)[:, 1]\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    auc = roc_auc_score(all_labels, all_probs)\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Test AUC: {auc:.4f}\")\n",
        "\n",
        "    # Create subdirectory for results including epochs\n",
        "    model_results_subdir = os.path.join(results_dir, f\"{backbone_name}_{temporal_pooling_method}_epochs{num_epochs}\")\n",
        "    os.makedirs(model_results_subdir, exist_ok=True)\n",
        "\n",
        "    # Save confusion matrix plot\n",
        "    fig, ax = plt.subplots()\n",
        "    cmd_display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Vertical\", \"Horizontal\"])\n",
        "    cmd_display.plot(ax=ax)\n",
        "    plt.title(f\"Confusion Matrix: {backbone_name}-{temporal_pooling_method}-Epochs{num_epochs}\")\n",
        "    plt.savefig(os.path.join(model_results_subdir, \"confusion_matrix.png\"))\n",
        "    plt.close(fig) # Close the figure to free memory\n",
        "\n",
        "    # Save numerical results\n",
        "    results_data = {\n",
        "        \"backbone\": backbone_name,\n",
        "        \"temporal_pooling\": temporal_pooling_method,\n",
        "        \"num_epochs\": num_epochs,\n",
        "        \"accuracy\": accuracy,\n",
        "        \"auc\": auc,\n",
        "        \"confusion_matrix\": cm.tolist() # Convert numpy array to list for JSON serialization\n",
        "    }\n",
        "    results_filepath = os.path.join(model_results_subdir, \"results.json\")\n",
        "    with open(results_filepath, \"w\") as f:\n",
        "        json.dump(results_data, f, indent=4)\n",
        "\n",
        "    print(f\"Results saved to {model_results_subdir}\")\n",
        "    return results_data # Return results for summary report\n",
        "\n",
        "\n",
        "def generate_summary_report(results_dir):\n",
        "    \"\"\"Generates a summary report from saved results.\"\"\"\n",
        "    print(\"\\nGenerating summary report...\")\n",
        "    summary_list = []\n",
        "\n",
        "    # Iterate through subdirectories in test_results_dir\n",
        "    for subdir_name in os.listdir(results_dir):\n",
        "        subdir_path = os.path.join(results_dir, subdir_name)\n",
        "        # Ensure it's a directory and contains results.json\n",
        "        if os.path.isdir(subdir_path):\n",
        "            results_filepath = os.path.join(subdir_path, \"results.json\")\n",
        "            if os.path.exists(results_filepath):\n",
        "                with open(results_filepath, \"r\") as f:\n",
        "                    results_data = json.load(f)\n",
        "                    summary_list.append(results_data)\n",
        "\n",
        "    if not summary_list:\n",
        "        print(\"No results found to generate a report.\")\n",
        "        return\n",
        "\n",
        "    # Create a pandas DataFrame for easier reporting\n",
        "    summary_df = pd.DataFrame(summary_list)\n",
        "    # Reorder columns for the summary report\n",
        "    summary_df = summary_df[['backbone', 'temporal_pooling', 'num_epochs', 'accuracy', 'auc']]\n",
        "    # Sort for better readability (optional)\n",
        "    summary_df = summary_df.sort_values(by=['backbone', 'temporal_pooling', 'num_epochs']).reset_index(drop=True)\n",
        "\n",
        "\n",
        "    print(\"\\n--- Model Performance Summary ---\")\n",
        "    print(summary_df.to_markdown(index=False)) # Use to_markdown for nice output in Colab\n",
        "\n",
        "    # Optional: Save summary to a file\n",
        "    summary_filepath = os.path.join(results_dir, \"summary_report.md\")\n",
        "    with open(summary_filepath, \"w\") as f:\n",
        "        f.write(\"### Model Performance Summary\\n\\n\")\n",
        "        f.write(summary_df.to_markdown(index=False))\n",
        "    print(f\"\\nSummary report saved to {summary_filepath}\")\n",
        "\n",
        "\n",
        "# --- Main Evaluation Pipeline ---\n",
        "\n",
        "trained_models_dir = \"trained_models\"\n",
        "test_results_dir = \"test_results\"\n",
        "# Ensure test_results_dir exists\n",
        "os.makedirs(test_results_dir, exist_ok=True)\n",
        "\n",
        "# Assuming 'config', 'test_loader', and 'device' are accessible from previous cells\n",
        "# Ensure device is set correctly\n",
        "device = config[\"training\"][\"device\"]\n",
        "if device == \"auto\":\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "else:\n",
        "  device = torch.device(device)\n",
        "\n",
        "\n",
        "print(f\"\\nStarting evaluation of models in {trained_models_dir}...\")\n",
        "\n",
        "evaluated_models_results = [] # To store results for the final report\n",
        "\n",
        "# Get the number of epochs from the config\n",
        "# num_epochs_from_config = config[\"training\"][\"epochs\"] # This is only needed if training here\n",
        "\n",
        "# Iterate through trained models\n",
        "for model_filename in os.listdir(trained_models_dir):\n",
        "    if model_filename.endswith(\".pth\"):\n",
        "        model_path = os.path.join(trained_models_dir, model_filename)\n",
        "        # Extract backbone and pooling from filename\n",
        "        try:\n",
        "            # Expected format: backbone_pooling_epochsX_model.pth OR backbone_pooling_model.pth\n",
        "            # Use regex for more flexible parsing\n",
        "            import re\n",
        "            match = re.match(r'([a-zA-Z0-9]+)_([a-zA-Z0-9]+)_?(epochs(\\d+))?_model\\.pth', model_filename)\n",
        "\n",
        "            if match:\n",
        "                backbone_name = match.group(1)\n",
        "                temporal_pooling_method = match.group(2)\n",
        "                loaded_epochs_str = match.group(4) # Extract the digits after 'epochs'\n",
        "                loaded_epochs = int(loaded_epochs_str) if loaded_epochs_str else None # Convert to int if present, else None\n",
        "                # Handle filenames without '_model' or 'epochs' if necessary, but current format includes '_model'\n",
        "\n",
        "            else:\n",
        "                 print(f\"Skipping file with unexpected name format: {model_filename}\")\n",
        "                 continue\n",
        "\n",
        "            print(f\"\\nLoading model: {model_filename}\")\n",
        "\n",
        "            # --- Model Loading ---\n",
        "            # Determine embedding_dim based on backbone name\n",
        "            if backbone_name == 'dino':\n",
        "                embedding_dim = 768\n",
        "            elif backbone_name == 'resnet18':\n",
        "                embedding_dim = 512\n",
        "            else:\n",
        "                # Default or handle unknown backbone\n",
        "                embedding_dim = config[\"model\"].get(\"embedding_dim\", 768)\n",
        "                print(f\"Warning: Unknown backbone '{backbone_name}', using default embedding_dim={embedding_dim}\")\n",
        "\n",
        "\n",
        "            model = VideoClassifier(\n",
        "                backbone_name=backbone_name,\n",
        "                temporal_pooling=temporal_pooling_method,\n",
        "                embedding_dim=embedding_dim, # Use dynamically determined embedding_dim\n",
        "                num_classes=config[\"model\"].get(\"num_classes\", 2),      # Use get with a default\n",
        "            ).to(device)\n",
        "\n",
        "            model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "            print(\"Model loaded successfully.\")\n",
        "            # --- End Model Loading ---\n",
        "\n",
        "            # Evaluate and save results\n",
        "            # Pass the loaded number of epochs to the evaluation and saving function\n",
        "            # Use loaded_epochs if available, otherwise use a default or indicate unknown\n",
        "            epochs_to_report = loaded_epochs if loaded_epochs is not None else \"unknown\"\n",
        "            model_results = evaluate_and_save_model_results(\n",
        "                model, test_loader, device, backbone_name, temporal_pooling_method, epochs_to_report, test_results_dir\n",
        "            )\n",
        "            evaluated_models_results.append(model_results)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing model file {model_filename}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "\n",
        "# Generate the final summary report\n",
        "generate_summary_report(test_results_dir)\n",
        "\n",
        "print(\"\\nEvaluation pipeline finished.\")"
      ],
      "metadata": {
        "id": "jzTQLB3GY6tw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c4b95ad-c9f5-456c-f82e-673566fa508c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting evaluation of models in trained_models...\n",
            "\n",
            "Loading model: dino_mean_model.pth\n",
            "Error processing model file dino_mean_model.pth: Backbone dino not implemented\n",
            "\n",
            "Loading model: dino_attention_model.pth\n",
            "Error processing model file dino_attention_model.pth: Backbone dino not implemented\n",
            "\n",
            "Loading model: resnet18_mean_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2193109311.py\", line 173, in <cell line: 0>\n",
            "    model = VideoClassifier(\n",
            "            ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Gonda_research_Hamutal_2024/Ossnat/VSD_FM_evaluate/VSD_foundation_model_evaluation/models/classifier.py\", line 8, in __init__\n",
            "    self.backbone = build_backbone(backbone_name, embedding_dim)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Gonda_research_Hamutal_2024/Ossnat/VSD_FM_evaluate/VSD_foundation_model_evaluation/models/backbones.py\", line 92, in build_backbone\n",
            "    raise ValueError(f\"Backbone {name} not implemented\")\n",
            "ValueError: Backbone dino not implemented\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2193109311.py\", line 173, in <cell line: 0>\n",
            "    model = VideoClassifier(\n",
            "            ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Gonda_research_Hamutal_2024/Ossnat/VSD_FM_evaluate/VSD_foundation_model_evaluation/models/classifier.py\", line 8, in __init__\n",
            "    self.backbone = build_backbone(backbone_name, embedding_dim)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Gonda_research_Hamutal_2024/Ossnat/VSD_FM_evaluate/VSD_foundation_model_evaluation/models/backbones.py\", line 92, in build_backbone\n",
            "    raise ValueError(f\"Backbone {name} not implemented\")\n",
            "ValueError: Backbone dino not implemented\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing model file resnet18_mean_model.pth: Error(s) in loading state_dict for VideoClassifier:\n",
            "\tsize mismatch for backbone.fc.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for backbone.fc.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for fc.weight: copying a param with shape torch.Size([2, 768]) from checkpoint, the shape in current model is torch.Size([2, 512]).\n",
            "\n",
            "Loading model: resnet18_attention_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2193109311.py\", line 180, in <cell line: 0>\n",
            "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 2624, in load_state_dict\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Error(s) in loading state_dict for VideoClassifier:\n",
            "\tsize mismatch for backbone.fc.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for backbone.fc.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for fc.weight: copying a param with shape torch.Size([2, 768]) from checkpoint, the shape in current model is torch.Size([2, 512]).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing model file resnet18_attention_model.pth: Error(s) in loading state_dict for VideoClassifier:\n",
            "\tsize mismatch for backbone.fc.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for backbone.fc.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for pooling.att.weight: copying a param with shape torch.Size([1, 768]) from checkpoint, the shape in current model is torch.Size([1, 512]).\n",
            "\tsize mismatch for fc.weight: copying a param with shape torch.Size([2, 768]) from checkpoint, the shape in current model is torch.Size([2, 512]).\n",
            "Skipping file with unexpected name format: dino2d_mean_epochs_5_model.pth\n",
            "Skipping file with unexpected name format: dino2d_attention_epochs_5_model.pth\n",
            "Skipping file with unexpected name format: resnet18_mean_epochs_5_model.pth\n",
            "Skipping file with unexpected name format: resnet18_attention_epochs_5_model.pth\n",
            "\n",
            "Generating summary report...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2193109311.py\", line 180, in <cell line: 0>\n",
            "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 2624, in load_state_dict\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Error(s) in loading state_dict for VideoClassifier:\n",
            "\tsize mismatch for backbone.fc.weight: copying a param with shape torch.Size([768, 512]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for backbone.fc.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for pooling.att.weight: copying a param with shape torch.Size([1, 768]) from checkpoint, the shape in current model is torch.Size([1, 512]).\n",
            "\tsize mismatch for fc.weight: copying a param with shape torch.Size([2, 768]) from checkpoint, the shape in current model is torch.Size([2, 512]).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Model Performance Summary ---\n",
            "| backbone   | temporal_pooling   | num_epochs   |   accuracy |      auc |\n",
            "|:-----------|:-------------------|:-------------|-----------:|---------:|\n",
            "| dino       | attention          | unknown      |   0.980769 | 0.986893 |\n",
            "| dino       | mean               | unknown      |   1        | 1        |\n",
            "\n",
            "Summary report saved to test_results/summary_report.md\n",
            "\n",
            "Evaluation pipeline finished.\n"
          ]
        }
      ]
    }
  ]
}